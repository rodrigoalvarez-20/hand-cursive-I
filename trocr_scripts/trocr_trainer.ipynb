{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 19:17:40.322184: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740014260.335999  530392 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740014260.341046  530392 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-19 19:17:40.355722: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "import pytorch_lightning as pl\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_CACHE = \"/home/ralvarez22/Documentos/llm_data/llm_cache\"\n",
    "TROCR_MODEL = \"microsoft/trocr-large-printed\" #\"/home/ralvarez22/Documentos/llm_data/llm_cache/models--microsoft--trocr-large-stage1/snapshots/3c8ead8dfda428d914334169380bb546f770a300\"\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "\n",
    "DATASETS_PATH = \"../hand-cursive-trocr\"\n",
    "TRAIN_FILE = \"train_metadata.json\"\n",
    "VALID_FILE = \"valid_metadata.json\"\n",
    "\n",
    "BATCH_SIZE = 8 # Modify in case of CUDA OUT OF MEMORY\n",
    "ACC_BATCH =  BATCH_SIZE * 5\n",
    "LOGGING_STEPS = ACC_BATCH #1000\n",
    "\n",
    "CKP_PATH = \"../finetuned/trocr\"\n",
    "#FINAL_MODEL_PATH = \"../finetuned/trocr/final\"\n",
    "MODEL_CODENAME = \"Seth_Lowell\" # Model Codename versioning\n",
    "MODEL_VERSION = 1 \n",
    "\n",
    "LOG_DIR = \"../logs/trocr\"\n",
    "\n",
    "EPOCHS = 5 # I use this value because it was only a Proof of concept test. With more Epochs, the accurancy (in theory) should be better\n",
    "LR = 1e-5 # All the tutorials recommend 4e-5 or 5e-5, but, I couldn't get a good model, the model stopped learning at the epoch 20 or 25 and the Loss Graph begun to raise instead of go down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cer_metric = evaluate.load(\"cer\")\n",
    "os.makedirs(CKP_PATH, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, processor):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # abrir la imagen y label\n",
    "        df_data = self.df.iloc[idx]\n",
    "        return (\n",
    "            Image.open(os.path.join(self.root_dir, df_data[\"image\"])).convert(\"RGB\"),\n",
    "            df_data[\"label\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DF = pd.read_json(os.path.join(DATASETS_PATH, TRAIN_FILE))\n",
    "#VALIDATION_DF = pd.read_json(os.path.join(DATASETS_PATH, VALID_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processor = TrOCRProcessor.from_pretrained(TROCR_MODEL, cache_dir=HF_CACHE, device_map=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_function(batch):\n",
    "    # Pad and process images and labels\n",
    "    batch_images = [x[0] for x in batch]\n",
    "    batch_labels = [x[1] for x in batch]\n",
    "    pixel_values = processor(batch_images, return_tensors=\"pt\").pixel_values.to(DEVICE)\n",
    "    labels = processor.tokenizer(\n",
    "        batch_labels, add_special_tokens=True, padding=True, return_tensors=\"pt\"\n",
    "    ).input_ids.to(DEVICE)\n",
    "    # Clone the labels to avoid modifications in the original tensor\n",
    "    input_labels = labels.clone()\n",
    "    # Convert the EOS token to a padding token\n",
    "    input_labels = torch.where(\n",
    "        input_labels == processor.tokenizer.eos_token_id,\n",
    "        processor.tokenizer.pad_token_id,\n",
    "        input_labels,\n",
    "    )\n",
    "    # Because I shifted 1 item to the right, I need to add an additional token to preserve the dimensions\n",
    "    to_concat = (\n",
    "        torch.empty((1, input_labels.shape[0]), dtype=torch.long, device=DEVICE)\n",
    "        .masked_fill(\n",
    "            torch.ones(input_labels.shape[0], dtype=torch.bool, device=DEVICE),\n",
    "            processor.tokenizer.pad_token_id,\n",
    "        )\n",
    "        .transpose(1, 0)\n",
    "    )\n",
    "    # This are the shifted labels\n",
    "    shifted_labels = torch.cat((labels[:, 1:], to_concat), dim=1)\n",
    "    # Create the Attention Mask for the decoder\n",
    "    # shifted_mask = torch.ones_like(shifted_labels, device=\"cuda\")\n",
    "    # The attention is: 0 for pad token (or tokens to ignore), 1 for the other values\n",
    "    shifted_mask = torch.where(\n",
    "        shifted_labels == processor.tokenizer.pad_token_id, 0, 1\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    encoding = {\n",
    "        \"pixel_values\": pixel_values.squeeze(),\n",
    "        \"labels\": batch_labels,\n",
    "        \"decoder_input\": input_labels,\n",
    "        \"shift_mask\": shifted_mask,\n",
    "        \"shifted_labels\": shifted_labels\n",
    "    }\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET = OCRDataset(DATASETS_PATH, TRAIN_DF, processor)\n",
    "#VALIDATION_DATASET = OCRDataset(DATASETS_PATH, VALIDATION_DF, processor)\n",
    "\n",
    "TRAIN_DATASET = DataLoader(dataset=TRAIN_DATASET, batch_size=BATCH_SIZE, collate_fn=collate_function, pin_memory=False)\n",
    "#VALIDATION_DATASET = DataLoader(dataset=VALIDATION_DATASET, batch_size=BATCH_SIZE, collate_fn=collate_function, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[ 0.9529,  0.9529,  0.9529,  ...,  0.9529,  0.9529,  0.9529],\n",
       "           [ 0.9529,  0.9529,  0.9529,  ...,  0.9529,  0.9529,  0.9529],\n",
       "           [ 0.9529,  0.9529,  0.9529,  ...,  0.9529,  0.9529,  0.9529],\n",
       "           ...,\n",
       "           [ 0.9137,  0.9137,  0.9137,  ...,  0.9608,  0.9608,  0.9608],\n",
       "           [ 0.9137,  0.9137,  0.9137,  ...,  0.9608,  0.9608,  0.9608],\n",
       "           [ 0.9137,  0.9137,  0.9137,  ...,  0.9608,  0.9608,  0.9608]],\n",
       " \n",
       "          [[ 0.9529,  0.9529,  0.9529,  ...,  0.9529,  0.9529,  0.9529],\n",
       "           [ 0.9529,  0.9529,  0.9529,  ...,  0.9529,  0.9529,  0.9529],\n",
       "           [ 0.9529,  0.9529,  0.9529,  ...,  0.9529,  0.9529,  0.9529],\n",
       "           ...,\n",
       "           [ 0.9137,  0.9137,  0.9137,  ...,  0.9608,  0.9608,  0.9608],\n",
       "           [ 0.9137,  0.9137,  0.9137,  ...,  0.9608,  0.9608,  0.9608],\n",
       "           [ 0.9137,  0.9137,  0.9137,  ...,  0.9608,  0.9608,  0.9608]],\n",
       " \n",
       "          [[ 0.9529,  0.9529,  0.9529,  ...,  0.9529,  0.9529,  0.9529],\n",
       "           [ 0.9529,  0.9529,  0.9529,  ...,  0.9529,  0.9529,  0.9529],\n",
       "           [ 0.9529,  0.9529,  0.9529,  ...,  0.9529,  0.9529,  0.9529],\n",
       "           ...,\n",
       "           [ 0.9137,  0.9137,  0.9137,  ...,  0.9608,  0.9608,  0.9608],\n",
       "           [ 0.9137,  0.9137,  0.9137,  ...,  0.9608,  0.9608,  0.9608],\n",
       "           [ 0.9137,  0.9137,  0.9137,  ...,  0.9608,  0.9608,  0.9608]]],\n",
       " \n",
       " \n",
       "         [[[ 0.9608,  0.9608,  0.9608,  ...,  0.9294,  0.9294,  0.9294],\n",
       "           [ 0.9608,  0.9608,  0.9608,  ...,  0.9294,  0.9294,  0.9294],\n",
       "           [ 0.9608,  0.9608,  0.9608,  ...,  0.9294,  0.9294,  0.9294],\n",
       "           ...,\n",
       "           [-0.1765, -0.1765, -0.1765,  ...,  0.9529,  0.9451,  0.9451],\n",
       "           [-0.2314, -0.2314, -0.2314,  ...,  0.9529,  0.9451,  0.9451],\n",
       "           [-0.2314, -0.2314, -0.2314,  ...,  0.9529,  0.9451,  0.9451]],\n",
       " \n",
       "          [[ 0.9608,  0.9608,  0.9608,  ...,  0.9294,  0.9294,  0.9294],\n",
       "           [ 0.9608,  0.9608,  0.9608,  ...,  0.9294,  0.9294,  0.9294],\n",
       "           [ 0.9608,  0.9608,  0.9608,  ...,  0.9294,  0.9294,  0.9294],\n",
       "           ...,\n",
       "           [-0.1765, -0.1765, -0.1765,  ...,  0.9529,  0.9451,  0.9451],\n",
       "           [-0.2314, -0.2314, -0.2314,  ...,  0.9529,  0.9451,  0.9451],\n",
       "           [-0.2314, -0.2314, -0.2314,  ...,  0.9529,  0.9451,  0.9451]],\n",
       " \n",
       "          [[ 0.9608,  0.9608,  0.9608,  ...,  0.9294,  0.9294,  0.9294],\n",
       "           [ 0.9608,  0.9608,  0.9608,  ...,  0.9294,  0.9294,  0.9294],\n",
       "           [ 0.9608,  0.9608,  0.9608,  ...,  0.9294,  0.9294,  0.9294],\n",
       "           ...,\n",
       "           [-0.1765, -0.1765, -0.1765,  ...,  0.9529,  0.9451,  0.9451],\n",
       "           [-0.2314, -0.2314, -0.2314,  ...,  0.9529,  0.9451,  0.9451],\n",
       "           [-0.2314, -0.2314, -0.2314,  ...,  0.9529,  0.9451,  0.9451]]],\n",
       " \n",
       " \n",
       "         [[[ 0.9843,  0.9843,  0.9843,  ...,  1.0000,  1.0000,  1.0000],\n",
       "           [ 0.9843,  0.9843,  0.9843,  ...,  1.0000,  1.0000,  1.0000],\n",
       "           [ 0.9843,  0.9843,  0.9843,  ...,  1.0000,  1.0000,  1.0000],\n",
       "           ...,\n",
       "           [ 0.9843,  0.9843,  0.9843,  ...,  0.9922,  0.9922,  0.9922],\n",
       "           [ 0.9843,  0.9843,  0.9843,  ...,  0.9922,  0.9922,  0.9922],\n",
       "           [ 0.9843,  0.9843,  0.9843,  ...,  0.9922,  0.9922,  0.9922]],\n",
       " \n",
       "          [[ 0.9843,  0.9843,  0.9843,  ...,  1.0000,  1.0000,  1.0000],\n",
       "           [ 0.9843,  0.9843,  0.9843,  ...,  1.0000,  1.0000,  1.0000],\n",
       "           [ 0.9843,  0.9843,  0.9843,  ...,  1.0000,  1.0000,  1.0000],\n",
       "           ...,\n",
       "           [ 0.9843,  0.9843,  0.9843,  ...,  0.9922,  0.9922,  0.9922],\n",
       "           [ 0.9843,  0.9843,  0.9843,  ...,  0.9922,  0.9922,  0.9922],\n",
       "           [ 0.9843,  0.9843,  0.9843,  ...,  0.9922,  0.9922,  0.9922]],\n",
       " \n",
       "          [[ 0.9843,  0.9843,  0.9843,  ...,  1.0000,  1.0000,  1.0000],\n",
       "           [ 0.9843,  0.9843,  0.9843,  ...,  1.0000,  1.0000,  1.0000],\n",
       "           [ 0.9843,  0.9843,  0.9843,  ...,  1.0000,  1.0000,  1.0000],\n",
       "           ...,\n",
       "           [ 0.9843,  0.9843,  0.9843,  ...,  0.9922,  0.9922,  0.9922],\n",
       "           [ 0.9843,  0.9843,  0.9843,  ...,  0.9922,  0.9922,  0.9922],\n",
       "           [ 0.9843,  0.9843,  0.9843,  ...,  0.9922,  0.9922,  0.9922]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 0.9686,  0.9686,  0.9686,  ...,  0.9765,  0.9765,  0.9765],\n",
       "           [ 0.9686,  0.9686,  0.9686,  ...,  0.9765,  0.9765,  0.9765],\n",
       "           [ 0.9686,  0.9686,  0.9686,  ...,  0.9765,  0.9765,  0.9765],\n",
       "           ...,\n",
       "           [ 0.9686,  0.9686,  0.9765,  ...,  0.9922,  0.9922,  0.9922],\n",
       "           [ 0.9686,  0.9686,  0.9765,  ...,  0.9922,  0.9922,  0.9922],\n",
       "           [ 0.9686,  0.9686,  0.9765,  ...,  0.9922,  0.9922,  0.9922]],\n",
       " \n",
       "          [[ 0.9686,  0.9686,  0.9686,  ...,  0.9765,  0.9765,  0.9765],\n",
       "           [ 0.9686,  0.9686,  0.9686,  ...,  0.9765,  0.9765,  0.9765],\n",
       "           [ 0.9686,  0.9686,  0.9686,  ...,  0.9765,  0.9765,  0.9765],\n",
       "           ...,\n",
       "           [ 0.9686,  0.9686,  0.9765,  ...,  0.9922,  0.9922,  0.9922],\n",
       "           [ 0.9686,  0.9686,  0.9765,  ...,  0.9922,  0.9922,  0.9922],\n",
       "           [ 0.9686,  0.9686,  0.9765,  ...,  0.9922,  0.9922,  0.9922]],\n",
       " \n",
       "          [[ 0.9686,  0.9686,  0.9686,  ...,  0.9765,  0.9765,  0.9765],\n",
       "           [ 0.9686,  0.9686,  0.9686,  ...,  0.9765,  0.9765,  0.9765],\n",
       "           [ 0.9686,  0.9686,  0.9686,  ...,  0.9765,  0.9765,  0.9765],\n",
       "           ...,\n",
       "           [ 0.9686,  0.9686,  0.9765,  ...,  0.9922,  0.9922,  0.9922],\n",
       "           [ 0.9686,  0.9686,  0.9765,  ...,  0.9922,  0.9922,  0.9922],\n",
       "           [ 0.9686,  0.9686,  0.9765,  ...,  0.9922,  0.9922,  0.9922]]],\n",
       " \n",
       " \n",
       "         [[[ 0.9451,  0.9451,  0.9451,  ...,  0.9608,  0.9608,  0.9608],\n",
       "           [ 0.9451,  0.9451,  0.9451,  ...,  0.9608,  0.9608,  0.9608],\n",
       "           [ 0.9451,  0.9451,  0.9451,  ...,  0.9608,  0.9608,  0.9608],\n",
       "           ...,\n",
       "           [ 0.9529,  0.9529,  0.9608,  ...,  0.9686,  0.9686,  0.9686],\n",
       "           [ 0.9529,  0.9529,  0.9608,  ...,  0.9686,  0.9686,  0.9686],\n",
       "           [ 0.9529,  0.9529,  0.9608,  ...,  0.9686,  0.9686,  0.9686]],\n",
       " \n",
       "          [[ 0.9451,  0.9451,  0.9451,  ...,  0.9608,  0.9608,  0.9608],\n",
       "           [ 0.9451,  0.9451,  0.9451,  ...,  0.9608,  0.9608,  0.9608],\n",
       "           [ 0.9451,  0.9451,  0.9451,  ...,  0.9608,  0.9608,  0.9608],\n",
       "           ...,\n",
       "           [ 0.9529,  0.9529,  0.9608,  ...,  0.9686,  0.9686,  0.9686],\n",
       "           [ 0.9529,  0.9529,  0.9608,  ...,  0.9686,  0.9686,  0.9686],\n",
       "           [ 0.9529,  0.9529,  0.9608,  ...,  0.9686,  0.9686,  0.9686]],\n",
       " \n",
       "          [[ 0.9451,  0.9451,  0.9451,  ...,  0.9608,  0.9608,  0.9608],\n",
       "           [ 0.9451,  0.9451,  0.9451,  ...,  0.9608,  0.9608,  0.9608],\n",
       "           [ 0.9451,  0.9451,  0.9451,  ...,  0.9608,  0.9608,  0.9608],\n",
       "           ...,\n",
       "           [ 0.9529,  0.9529,  0.9608,  ...,  0.9686,  0.9686,  0.9686],\n",
       "           [ 0.9529,  0.9529,  0.9608,  ...,  0.9686,  0.9686,  0.9686],\n",
       "           [ 0.9529,  0.9529,  0.9608,  ...,  0.9686,  0.9686,  0.9686]]],\n",
       " \n",
       " \n",
       "         [[[ 0.9765,  0.9765,  0.9765,  ...,  0.9843,  0.9843,  0.9843],\n",
       "           [ 0.9765,  0.9765,  0.9765,  ...,  0.9843,  0.9843,  0.9843],\n",
       "           [ 0.9765,  0.9765,  0.9765,  ...,  0.9843,  0.9843,  0.9843],\n",
       "           ...,\n",
       "           [ 0.9843,  0.9843,  0.9843,  ...,  0.9529,  0.9529,  0.9529],\n",
       "           [ 0.9843,  0.9843,  0.9843,  ...,  0.9529,  0.9529,  0.9529],\n",
       "           [ 0.9843,  0.9843,  0.9843,  ...,  0.9529,  0.9529,  0.9529]],\n",
       " \n",
       "          [[ 0.9765,  0.9765,  0.9765,  ...,  0.9843,  0.9843,  0.9843],\n",
       "           [ 0.9765,  0.9765,  0.9765,  ...,  0.9843,  0.9843,  0.9843],\n",
       "           [ 0.9765,  0.9765,  0.9765,  ...,  0.9843,  0.9843,  0.9843],\n",
       "           ...,\n",
       "           [ 0.9843,  0.9843,  0.9843,  ...,  0.9529,  0.9529,  0.9529],\n",
       "           [ 0.9843,  0.9843,  0.9843,  ...,  0.9529,  0.9529,  0.9529],\n",
       "           [ 0.9843,  0.9843,  0.9843,  ...,  0.9529,  0.9529,  0.9529]],\n",
       " \n",
       "          [[ 0.9765,  0.9765,  0.9765,  ...,  0.9843,  0.9843,  0.9843],\n",
       "           [ 0.9765,  0.9765,  0.9765,  ...,  0.9843,  0.9843,  0.9843],\n",
       "           [ 0.9765,  0.9765,  0.9765,  ...,  0.9843,  0.9843,  0.9843],\n",
       "           ...,\n",
       "           [ 0.9843,  0.9843,  0.9843,  ...,  0.9529,  0.9529,  0.9529],\n",
       "           [ 0.9843,  0.9843,  0.9843,  ...,  0.9529,  0.9529,  0.9529],\n",
       "           [ 0.9843,  0.9843,  0.9843,  ...,  0.9529,  0.9529,  0.9529]]]],\n",
       "        device='cuda:0'),\n",
       " 'labels': ['db', 'ochenta', 'y', 'y', 'veintiséis', 'ocho', 'ochenta', 'y'],\n",
       " 'decoder_input': tensor([[    0, 33845,     1,     1,     1,     1,     1],\n",
       "         [    0,  1975, 37754,   102,     1,     1,     1],\n",
       "         [    0,   219,     1,     1,     1,     1,     1],\n",
       "         [    0,   219,     1,     1,     1,     1,     1],\n",
       "         [    0,   548,  2544,   354,  1140,   354,     1],\n",
       "         [    0,  4306,   139,     1,     1,     1,     1],\n",
       "         [    0,  1975, 37754,   102,     1,     1,     1],\n",
       "         [    0,   219,     1,     1,     1,     1,     1]], device='cuda:0'),\n",
       " 'shift_mask': tensor([[1, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0],\n",
       "         [1, 1, 1, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 0, 0, 0, 0, 0]], device='cuda:0'),\n",
       " 'shifted_labels': tensor([[33845,     2,     1,     1,     1,     1,     1],\n",
       "         [ 1975, 37754,   102,     2,     1,     1,     1],\n",
       "         [  219,     2,     1,     1,     1,     1,     1],\n",
       "         [  219,     2,     1,     1,     1,     1,     1],\n",
       "         [  548,  2544,   354,  1140,   354,     2,     1],\n",
       "         [ 4306,   139,     2,     1,     1,     1,     1],\n",
       "         [ 1975, 37754,   102,     2,     1,     1,     1],\n",
       "         [  219,     2,     1,     1,     1,     1,     1]], device='cuda:0')}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(TRAIN_DATASET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 7796\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples:\", len(TRAIN_DATASET))\n",
    "#print(\"Number of validation examples:\", len(VALIDATION_DATASET))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandCursiveTrOCR(pl.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path,\n",
    "        image_processor,\n",
    "        train_dataset,\n",
    "        eval_dataset=None,\n",
    "        learning_rate=4e-5,\n",
    "        weight_decay=0.1,\n",
    "        cache_dir=\"\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = VisionEncoderDecoderModel.from_pretrained(\n",
    "            model_path, cache_dir=cache_dir\n",
    "        )\n",
    "        \n",
    "        self.model.train()\n",
    "        \n",
    "        self.image_processor = image_processor\n",
    "\n",
    "        self.model.generation_config.decoder_start_token_id = (\n",
    "            self.image_processor.tokenizer.bos_token_id\n",
    "        )\n",
    "\n",
    "        self.model.generation_config.temperature = 0.4\n",
    "        self.model.generation_config.max_length = 200\n",
    "        self.model.generation_config.do_sample = True\n",
    "\n",
    "        self.model.config.decoder.bos_token_id = (\n",
    "            self.image_processor.tokenizer.bos_token_id\n",
    "        )\n",
    "        self.model.config.decoder.decoder_start_token_id = (\n",
    "            self.image_processor.tokenizer.bos_token_id\n",
    "        )\n",
    "        self.model.config.decoder.eos_token_id = (\n",
    "            self.image_processor.tokenizer.eos_token_id\n",
    "        )\n",
    "        self.model.config.decoder.pad_token_id = (\n",
    "            self.image_processor.tokenizer.pad_token_id\n",
    "        )\n",
    "        self.model.config.encoder.bos_token_id = (\n",
    "            self.image_processor.tokenizer.bos_token_id\n",
    "        )\n",
    "        self.model.config.encoder.decoder_start_token_id = (\n",
    "            self.image_processor.tokenizer.bos_token_id\n",
    "        )\n",
    "        self.model.config.encoder.eos_token_id = (\n",
    "            self.image_processor.tokenizer.eos_token_id\n",
    "        )\n",
    "        self.model.config.vocab_size = self.image_processor.tokenizer.vocab_size\n",
    "\n",
    "        self.criterion = torch.nn.CrossEntropyLoss(\n",
    "            ignore_index=self.image_processor.tokenizer.pad_token_id\n",
    "        )\n",
    "        self.train_dataset = train_dataset\n",
    "        self.evaluation_dataset = eval_dataset\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, pixel_values, decoder_input_ids, decoder_mask=None):\n",
    "        return self.model.forward(pixel_values, decoder_input_ids, decoder_mask)\n",
    "\n",
    "    def common_step(self, batch):\n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        decoder_inputs = batch[\"decoder_input\"]\n",
    "        shifted_mask = batch[\"shift_mask\"]\n",
    "        shifted_labels = batch[\"shifted_labels\"]\n",
    "        model_output = self.forward(pixel_values, decoder_inputs, shifted_mask)\n",
    "        logits = model_output.logits\n",
    "        loss = self.criterion(\n",
    "            logits.contiguous().view(-1, self.model.config.decoder.vocab_size),\n",
    "            shifted_labels.contiguous().view(-1),\n",
    "        )\n",
    "        return loss, loss.item()\n",
    "\n",
    "    #def compute_cer_metric(self, batch):\n",
    "    #    pixel_values = batch[\"pixel_values\"]\n",
    "    #    gt_labels = batch[\"labels\"]\n",
    "    #    model_predictions = self.model.generate(pixel_values)\n",
    "    #    predicted_strings = self.image_processor.tokenizer.batch_decode(model_predictions, skip_special_tokens=True)\n",
    "    #    return cer_metric.compute(predictions=predicted_strings, references=gt_labels)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        loss, loss_value = self.common_step(batch)\n",
    "        self.log(\"train_loss\", loss_value)\n",
    "        return loss\n",
    "\n",
    "    #def validation_step(self, batch):\n",
    "    #    loss, loss_value = self.common_step(batch)\n",
    "    #    self.log(\"validation_loss\", loss_value)\n",
    "    #    cer_value = self.compute_cer_metric(batch)\n",
    "    #    self.log(\"validation_cer\", cer_value)\n",
    "    #    return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_dataset\n",
    "\n",
    "    #def val_dataloader(self):\n",
    "    #    return self.evaluation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"transformers_version\": \"4.46.3\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 1024,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": false,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-large-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = HandCursiveTrOCR(\n",
    "    TROCR_MODEL,\n",
    "    processor,\n",
    "    TRAIN_DATASET,\n",
    "    #VALIDATION_DATASET,\n",
    "    learning_rate=LR,\n",
    "    cache_dir=HF_CACHE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params for the logger\n",
    "hyperparams = {\n",
    "    \"model_type\": \"TrOCR\",\n",
    "    \"model_name\": \"microsoft/trocr-large-stage1\",\n",
    "    \"codename\": MODEL_CODENAME,\n",
    "    \"version\": MODEL_VERSION,\n",
    "    \"model_learning_rate\": LR,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"acc_grad_batches\": ACC_BATCH,\n",
    "    \"batch_size\": BATCH_SIZE\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 609.169408\n",
      "Trainable params: 609.169408 M\n"
     ]
    }
   ],
   "source": [
    "trocr_total_params = sum(p.numel() for p in model.parameters())\n",
    "trocr_train_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total params: {}\\nTrainable params: {} M\".format(trocr_total_params / 1e6, trocr_train_params/ 1e6))\n",
    "hyperparams[\"total_params\"] = trocr_total_params\n",
    "hyperparams[\"trainable_params\"] = trocr_train_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join(LOG_DIR, MODEL_CODENAME, \"version_{}\".format(MODEL_VERSION))\n",
    "shutil.rmtree(log_path, ignore_errors=True)\n",
    "# I used Tensorboard Logger. If you too, please make sure to initiate the TB instance\n",
    "logger = pl.loggers.TensorBoardLogger(save_dir=LOG_DIR, version=MODEL_VERSION, name=MODEL_CODENAME)\n",
    "logger.log_hyperparams(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name      | Type                      | Params | Mode \n",
      "----------------------------------------------------------------\n",
      "0 | model     | VisionEncoderDecoderModel | 609 M  | train\n",
      "1 | criterion | CrossEntropyLoss          | 0      | train\n",
      "----------------------------------------------------------------\n",
      "609 M     Trainable params\n",
      "0         Non-trainable params\n",
      "609 M     Total params\n",
      "2,436.678 Total estimated model params size (MB)\n",
      "656       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ralvarez22/server/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea5d5ebd05e546fabe06f13c01eac902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(devices=1, accelerator=\"gpu\", max_epochs=EPOCHS, precision=\"bf16-mixed\", accumulate_grad_batches=ACC_BATCH, log_every_n_steps=LOGGING_STEPS, val_check_interval=LOGGING_STEPS, logger=logger)\n",
    "# Run the training Cycle and log the metrics\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the final model\n",
    "FINAL_CKP_PATH = os.path.join(CKP_PATH, MODEL_CODENAME, \"V_{}\".format(MODEL_VERSION))\n",
    "# By default it saves the Safetensors type\n",
    "model.model.save_pretrained(FINAL_CKP_PATH)\n",
    "processor.save_pretrained(FINAL_CKP_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyFormers + CUDA",
   "language": "python",
   "name": "dev-kernel-v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
