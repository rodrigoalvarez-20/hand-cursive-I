{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "import pytorch_lightning as pl\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "from clearml import Task\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is in case to use ClearML (Local with docker) to Log the metrics\n",
    "%env CLEARML_WEB_HOST=http://localhost:8080\n",
    "%env CLEARML_API_HOST=http://localhost:8008\n",
    "%env CLEARML_FILES_HOST=http://localhost:8081\n",
    "%env CLEARML_API_ACCESS_KEY=AEBY191O3R1U4SGBDPLA\n",
    "%env CLEARML_API_SECRET_KEY=OVvAzcKHtSfqP95jjMHgmgAvzDcSKIKRt5wv1hE1PerO5D3uiT\n",
    "%env CLEARML_LOG_MODEL=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_CACHE = \"/home/ralvarez22/Documentos/llm_data/llm_cache\"\n",
    "TROCR_MODEL = \"/home/ralvarez22/Documentos/llm_data/llm_cache/models--microsoft--trocr-large-stage1/snapshots/3c8ead8dfda428d914334169380bb546f770a300\"\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "\n",
    "DATASETS_PATH = \"../hand-cursive-trocr\"\n",
    "TRAIN_FILE = \"train_metadata.json\"\n",
    "VALID_FILE = \"valid_metadata.json\"\n",
    "\n",
    "BATCH_SIZE = 8 # Modify in case of CUDA OUT OF MEMORY\n",
    "ACC_BATCH =  BATCH_SIZE * 4\n",
    "LOGGING_STEPS = 1000\n",
    "\n",
    "CKP_PATH = \"../finetuned/trocr\"\n",
    "FINAL_MODEL_PATH = \"../finetuned/trocr/final\"\n",
    "MODEL_CODENAME = \"Terminus\" # Model Codename versioning\n",
    "MODEL_VERSION = 1 \n",
    "\n",
    "LOG_DIR = \"../clearml_logs_trocr\"\n",
    "\n",
    "EPOCHS = 5 # I use this value because it was only a Proof of concept test. With more Epochs, the accurancy (in theory) should be better\n",
    "LR = 1e-5 # All the tutorials recommend 4e-5 or 5e-5, but, I couldn't get a good model, the model stopped learning at the epoch 20 or 25 and the Loss Graph begun to raise instead of go down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cer_metric = evaluate.load(\"cer\")\n",
    "os.makedirs(CKP_PATH, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, processor):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # abrir la imagen y label\n",
    "        df_data = self.df.iloc[idx]\n",
    "        return (\n",
    "            Image.open(os.path.join(self.root_dir, df_data[\"image\"])).convert(\"RGB\"),\n",
    "            df_data[\"label\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DF = pd.read_json(os.path.join(DATASETS_PATH, TRAIN_FILE))\n",
    "VALIDATION_DF = pd.read_json(os.path.join(DATASETS_PATH, VALID_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processor = TrOCRProcessor.from_pretrained(TROCR_MODEL, cache_dir=HF_CACHE, device_map=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_function(batch):\n",
    "    # Pad and process images and labels\n",
    "    batch_images = [x[0] for x in batch]\n",
    "    batch_labels = [x[1] for x in batch]\n",
    "    pixel_values = processor(batch_images, return_tensors=\"pt\").pixel_values.to(DEVICE)\n",
    "    labels = processor.tokenizer(\n",
    "        batch_labels, add_special_tokens=True, padding=True, return_tensors=\"pt\"\n",
    "    ).input_ids.to(DEVICE)\n",
    "    # Clone the labels to avoid modifications in the original tensor\n",
    "    input_labels = labels.clone()\n",
    "    # Convert the EOS token to a padding token\n",
    "    input_labels = torch.where(\n",
    "        input_labels == processor.tokenizer.eos_token_id,\n",
    "        processor.tokenizer.pad_token_id,\n",
    "        input_labels,\n",
    "    )\n",
    "    # Because I shifted 1 item to the right, I need to add an additional token to preserve the dimensions\n",
    "    to_concat = (\n",
    "        torch.empty((1, input_labels.shape[0]), dtype=torch.long, device=DEVICE)\n",
    "        .masked_fill(\n",
    "            torch.ones(input_labels.shape[0], dtype=torch.bool, device=DEVICE),\n",
    "            processor.tokenizer.pad_token_id,\n",
    "        )\n",
    "        .transpose(1, 0)\n",
    "    )\n",
    "    # This are the shifted labels\n",
    "    shifted_labels = torch.cat((labels[:, 1:], to_concat), dim=1)\n",
    "    # Create the Attention Mask for the decoder\n",
    "    # shifted_mask = torch.ones_like(shifted_labels, device=\"cuda\")\n",
    "    # The attention is: 0 for pad token (or tokens to ignore), 1 for the other values\n",
    "    shifted_mask = torch.where(\n",
    "        shifted_labels == processor.tokenizer.pad_token_id, 0, 1\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    encoding = {\n",
    "        \"pixel_values\": pixel_values.squeeze(),\n",
    "        \"labels\": batch_labels,\n",
    "        \"decoder_input\": input_labels,\n",
    "        \"shift_mask\": shifted_mask,\n",
    "        \"shifted_labels\": shifted_labels\n",
    "    }\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET = OCRDataset(DATASETS_PATH, TRAIN_DF, processor)\n",
    "VALIDATION_DATASET = OCRDataset(DATASETS_PATH, VALIDATION_DF, processor)\n",
    "\n",
    "TRAIN_DATASET = DataLoader(dataset=TRAIN_DATASET, batch_size=BATCH_SIZE, collate_fn=collate_function, pin_memory=False)\n",
    "VALIDATION_DATASET = DataLoader(dataset=VALIDATION_DATASET, batch_size=BATCH_SIZE, collate_fn=collate_function, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(TRAIN_DATASET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of training examples:\", len(TRAIN_DATASET))\n",
    "print(\"Number of validation examples:\", len(VALIDATION_DATASET))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandCursiveTrOCR(pl.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path,\n",
    "        image_processor,\n",
    "        train_dataset,\n",
    "        eval_dataset=None,\n",
    "        learning_rate=4e-5,\n",
    "        weight_decay=0.1,\n",
    "        cache_dir=\"\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = VisionEncoderDecoderModel.from_pretrained(\n",
    "            model_path, cache_dir=cache_dir\n",
    "        )\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "        self.model.generation_config.decoder_start_token_id = (\n",
    "            self.image_processor.tokenizer.bos_token_id\n",
    "        )\n",
    "\n",
    "        self.model.generation_config.temperature = 0.4\n",
    "        self.model.generation_config.max_length = 200\n",
    "        self.model.generation_config.do_sample = True\n",
    "\n",
    "        self.model.config.decoder.bos_token_id = (\n",
    "            self.image_processor.tokenizer.bos_token_id\n",
    "        )\n",
    "        self.model.config.decoder.decoder_start_token_id = (\n",
    "            self.image_processor.tokenizer.bos_token_id\n",
    "        )\n",
    "        self.model.config.decoder.eos_token_id = (\n",
    "            self.image_processor.tokenizer.eos_token_id\n",
    "        )\n",
    "        self.model.config.decoder.pad_token_id = (\n",
    "            self.image_processor.tokenizer.pad_token_id\n",
    "        )\n",
    "        self.model.config.encoder.bos_token_id = (\n",
    "            self.image_processor.tokenizer.bos_token_id\n",
    "        )\n",
    "        self.model.config.encoder.decoder_start_token_id = (\n",
    "            self.image_processor.tokenizer.bos_token_id\n",
    "        )\n",
    "        self.model.config.encoder.eos_token_id = (\n",
    "            self.image_processor.tokenizer.eos_token_id\n",
    "        )\n",
    "        self.model.config.vocab_size = self.image_processor.tokenizer.vocab_size\n",
    "\n",
    "        self.criterion = torch.nn.CrossEntropyLoss(\n",
    "            ignore_index=self.image_processor.tokenizer.pad_token_id\n",
    "        )\n",
    "        self.train_dataset = train_dataset\n",
    "        self.evaluation_dataset = eval_dataset\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, pixel_values, decoder_input_ids, decoder_mask=None):\n",
    "        return self.model.forward(pixel_values, decoder_input_ids, decoder_mask)\n",
    "\n",
    "    def common_step(self, batch):\n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        decoder_inputs = batch[\"decoder_input\"]\n",
    "        shifted_mask = batch[\"shift_mask\"]\n",
    "        shifted_labels = batch[\"shifted_labels\"]\n",
    "        model_output = self.forward(pixel_values, decoder_inputs, shifted_mask)\n",
    "        logits = model_output.logits\n",
    "        loss = self.criterion(\n",
    "            logits.contiguous().view(-1, self.model.config.decoder.vocab_size),\n",
    "            shifted_labels.contiguous().view(-1),\n",
    "        )\n",
    "        return loss, loss.item()\n",
    "\n",
    "    def compute_cer_metric(self, batch):\n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        gt_labels = batch[\"labels\"]\n",
    "        model_predictions = self.model.generate(pixel_values)\n",
    "        predicted_strings = self.image_processor.tokenizer.batch_decode(model_predictions, skip_special_tokens=True)\n",
    "        return cer_metric.compute(predictions=predicted_strings, references=gt_labels)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        loss, loss_value = self.common_step(batch)\n",
    "        self.log(\"train_loss\", loss_value)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        loss, loss_value = self.common_step(batch)\n",
    "        self.log(\"validation_loss\", loss_value)\n",
    "        cer_value = self.compute_cer_metric(batch)\n",
    "        self.log(\"validation_cer\", cer_value)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_dataset\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.evaluation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HandCursiveTrOCR(\n",
    "    TROCR_MODEL,\n",
    "    processor,\n",
    "    TRAIN_DATASET,\n",
    "    VALIDATION_DATASET,\n",
    "    learning_rate=LR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params for the logger\n",
    "hyperparams = {\n",
    "    \"model_type\": \"TrOCR\",\n",
    "    \"model_name\": \"microsoft/trocr-large-stage1\",\n",
    "    \"codename\": MODEL_CODENAME,\n",
    "    \"version\": MODEL_VERSION,\n",
    "    \"model_learning_rate\": LR,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"acc_grad_batches\": ACC_BATCH,\n",
    "    \"batch_size\": BATCH_SIZE\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trocr_total_params = sum(p.numel() for p in model.parameters())\n",
    "trocr_train_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total params: {}\\nTrainable params: {} M\".format(trocr_total_params / 1e6, trocr_train_params/ 1e6))\n",
    "hyperparams[\"total_params\"] = trocr_total_params\n",
    "hyperparams[\"trainable_params\"] = trocr_train_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsk_name = \"{}_V_{}\".format(MODEL_CODENAME, str(MODEL_VERSION))\n",
    "task = Task.init(task_name=tsk_name, project_name=\"HandCursive-I\")\n",
    "task.set_parameters(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join(LOG_DIR, MODEL_CODENAME, \"version_{}\".format(MODEL_VERSION))\n",
    "shutil.rmtree(log_path, ignore_errors=True)\n",
    "# I used Tensorboard Logger. If you too, please make sure to initiate the TB instance\n",
    "logger = pl.loggers.TensorBoardLogger(save_dir=LOG_DIR, version=MODEL_VERSION, name=MODEL_CODENAME)\n",
    "logger.log_hyperparams(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(devices=1, accelerator=\"gpu\", max_epochs=EPOCHS, precision=\"bf16-mixed\", accumulate_grad_batches=ACC_BATCH, log_every_n_steps=LOGGING_STEPS, val_check_interval=LOGGING_STEPS, logger=logger)\n",
    "# Run the training Cycle and log the metrics\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "FINAL_CKP_PATH = os.path.join(CKP_PATH, MODEL_CODENAME, \"V_{}\".format(MODEL_VERSION))\n",
    "# By default it saves the Safetensors type\n",
    "model.model.save_pretrained(FINAL_CKP_PATH)\n",
    "processor.save_pretrained(FINAL_CKP_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyFormers + CUDA",
   "language": "python",
   "name": "dev-kernel-v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
