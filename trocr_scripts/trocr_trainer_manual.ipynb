{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TROCR Pytorch Finetuning with CUSTOM DATASET\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook was used for finetuning Microsoft/trocr-large-stage1 base model (I dont use the Handwritten finetuned version to avoid language mistakes)\n",
    "\n",
    "I used the same dataset of the DETR project, but, insted of downloading in COCO JSON format, I downloaded in XML format and parse it with \"xml_workbench.ipynb\" lab\n",
    "\n",
    "The difference with the DETR dataset, this dataset contains all the original labels (~790 labels).\n",
    "\n",
    "The structure of the dataset is: \"image_path\" and \"label\" (in text)\n",
    "\n",
    "The reason for making the train cycle \"manually\" was for problems with GPU memory (Out of memory), to solve it, I implemented the cycle from scratch based on the CausalLLM Finetuning (SFTTrainer)\n",
    "\n",
    "Basically, I used the \"Right Shift\" technique.\n",
    "\n",
    "The input for the Encoder are the pixel values, the input for the Decoder is the target text including BOS token and excluding the EOS token, with padding.\n",
    "\n",
    "To ilustrate:\n",
    "\n",
    "- `This is the text`\n",
    "- `<bos> This is the text`\n",
    "- `This is the text <eos>`\n",
    "\n",
    "Author: Rodrigo Alvarez\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "from clearml import Task\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import torch\n",
    "import fastwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is in case to use ClearML (Local with docker) to Log the metrics\n",
    "%env CLEARML_WEB_HOST=http://localhost:8080\n",
    "%env CLEARML_API_HOST=http://localhost:8008\n",
    "%env CLEARML_FILES_HOST=http://localhost:8081\n",
    "%env CLEARML_API_ACCESS_KEY=AEBY191O3R1U4SGBDPLA\n",
    "%env CLEARML_API_SECRET_KEY=OVvAzcKHtSfqP95jjMHgmgAvzDcSKIKRt5wv1hE1PerO5D3uiT\n",
    "%env CLEARML_LOG_MODEL=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_CACHE = \"/home/ralvarez22/Documentos/llm_data/llm_cache\"\n",
    "TROCR_MODEL = \"/home/ralvarez22/Documentos/llm_data/llm_cache/models--microsoft--trocr-large-stage1/snapshots/3c8ead8dfda428d914334169380bb546f770a300\"\n",
    "\n",
    "DATASET_PATH = \"../hand-cursive-trocr\"\n",
    "\n",
    "TRAIN_METADATA_FILE = \"train_metadata.json\"\n",
    "VALID_METADATA_FILE = \"valid_metadata.json\"\n",
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the processor and the model\n",
    "processor = TrOCRProcessor.from_pretrained(\n",
    "    TROCR_MODEL, cache_dir=HF_CACHE, device_map=DEVICE\n",
    ")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\n",
    "    TROCR_MODEL, cache_dir=HF_CACHE, device_map=DEVICE\n",
    ")\n",
    "# In every tutorial I found, they dont modify the config of the processor and model\n",
    "# This configuration sets the special tokens for a valid Training and Inference use\n",
    "# Please make sure to set the decoder_start_token_id to the tokenizer bos_token_id\n",
    "# In some cases, the bos_token_id is the eos_token_id. This results in NO generation, because the end-of-sequence\n",
    "model.generation_config.decoder_start_token_id = processor.tokenizer.bos_token_id\n",
    "model.generation_config.max_new_tokens = 120\n",
    "model.generation_config.temperature = 0.4\n",
    "model.generation_config.do_sample = True\n",
    "model.config.decoder.bos_token_id = processor.tokenizer.bos_token_id\n",
    "model.config.decoder.decoder_start_token_id = processor.tokenizer.bos_token_id\n",
    "model.config.decoder.eos_token_id = processor.tokenizer.eos_token_id\n",
    "model.config.decoder.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.encoder.bos_token_id = processor.tokenizer.bos_token_id\n",
    "model.config.encoder.decoder_start_token_id = processor.tokenizer.bos_token_id\n",
    "model.config.encoder.eos_token_id = processor.tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8  # Modify in case of CUDA OUT OF MEMORY\n",
    "MODEL_USED = \"microsoft/trocr-large-stage1\"\n",
    "CKP_PATH = \"../finetuned/trocr\"\n",
    "FINAL_MODEL_PATH = \"../finetuned/trocr\"\n",
    "MODEL_CODENAME = \"Terminus\"  # Model Codename versioning\n",
    "MODEL_VERSION = 2\n",
    "SAVE_CKP_EVERY_N_EPOCHS = 2\n",
    "EVAL_EVERY_N_EPOCHS = 1\n",
    "MAX_TRAIN_ITEMS = -1\n",
    "EPOCHS = 5  # I use this value because it was only a Proof of concept test. With more Epochs, the accurancy (in theory) should be better\n",
    "LR = 1e-5  # All the tutorials recommend 4e-5 or 5e-5, but, I couldn't get a good model, the model stopped learning at the epoch 20 or 25 and the Loss Graph begun to raise instead of go down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to manually chunk the data\n",
    "def divide_chunks(l, n):\n",
    "    # looping till length l\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i : i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metadata file\n",
    "train_dataset_metadata = json.load(\n",
    "    open(os.path.join(DATASET_PATH, TRAIN_METADATA_FILE), \"r\")\n",
    ")\n",
    "eval_dataset_metadata = json.load(open(os.path.join(DATASET_PATH, VALID_METADATA_FILE), \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAX_TRAIN_ITEMS > 0:\n",
    "    train_dataset_metadata = train_dataset_metadata[:MAX_TRAIN_ITEMS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the chunks\n",
    "train_dataset = list(divide_chunks(train_dataset_metadata, BATCH_SIZE))\n",
    "evaluation_dataset = list(divide_chunks(eval_dataset_metadata, BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_info = {\n",
    "    \"type\": \"TROCR Cursive Handwritten\",\n",
    "    \"codename\": MODEL_CODENAME,\n",
    "    \"version\": MODEL_VERSION,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"learning_rate\": LR,\n",
    "    \"dataset\": \"HandCursive-I\",\n",
    "    \"model\": MODEL_USED,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trocr_total_params = sum(p.numel() for p in model.parameters())\n",
    "trocr_train_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\n",
    "    \"Total params: {}\\nTrainable params: {} M\".format(\n",
    "        trocr_total_params / 1e6, trocr_train_params / 1e6\n",
    "    )\n",
    ")\n",
    "log_info[\"total_params\"] = trocr_total_params\n",
    "log_info[\"trainable_params\"] = trocr_train_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsk_name = \"{}_V{}\".format(MODEL_CODENAME, str(MODEL_VERSION))\n",
    "task = Task.init(task_name=tsk_name, project_name=\"HandCursive-I\")\n",
    "task.set_parameters(log_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the Loss Function (CrossEntropy) and the Optimizer (AdamW)\n",
    "# I set the ignore_index to the tokenizer pad token to avoid bad calculations\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=processor.tokenizer.pad_token_id)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"{}/V_{}\".format(MODEL_CODENAME, MODEL_VERSION)\n",
    "epochs_path = os.path.join(CKP_PATH, model_name)\n",
    "print(\"Saving chekpoints to {}\".format(epochs_path))\n",
    "os.makedirs(epochs_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliar function to open the image and load the pixel values\n",
    "def load_and_process_images(images_chunk, troc_proc):\n",
    "    proc_chunk = []\n",
    "    for x in images_chunk:\n",
    "        proc_chunk.append(\n",
    "            troc_proc(\n",
    "                Image.open(os.path.join(DATASET_PATH, x)).convert(\"RGB\"),\n",
    "                return_tensors=\"pt\",\n",
    "            ).pixel_values.to(DEVICE)\n",
    "        )\n",
    "    # Use squeeze to eliminate the single array dimension of every item\n",
    "    return torch.stack(proc_chunk, 0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"training step\" function\n",
    "def train_step(trocr_model: VisionEncoderDecoderModel, dataset, ls_fn, optim):\n",
    "    losses = 0  # Accumulation of loss for every epoch\n",
    "    trocr_model.train()\n",
    "    for chunk in tqdm(dataset):\n",
    "        chunk_images = [x[\"image\"] for x in chunk]\n",
    "        labels = [x[\"label\"] for x in chunk]\n",
    "        chunk_images = load_and_process_images(\n",
    "            chunk_images, processor\n",
    "        )  # Process the batch images and get the batch pixels\n",
    "        # Tokenize the labels\n",
    "        labels = processor.tokenizer(\n",
    "            labels, add_special_tokens=True, return_tensors=\"pt\", padding=True\n",
    "        )[\"input_ids\"].to(DEVICE)\n",
    "        # Clone the labels to avoid modifications in the original tensor\n",
    "        input_labels = labels.clone()\n",
    "        # Convert the EOS token to a padding token\n",
    "        input_labels = torch.where(\n",
    "            input_labels == processor.tokenizer.eos_token_id,\n",
    "            processor.tokenizer.pad_token_id,\n",
    "            input_labels,\n",
    "        )\n",
    "        # Because I shifted 1 item to the right, I need to add an additional token to preserve the dimensions\n",
    "        to_concat = (\n",
    "            torch.empty((1, input_labels.shape[0]), dtype=torch.long, device=DEVICE)\n",
    "            .masked_fill(\n",
    "                torch.ones(input_labels.shape[0], dtype=torch.bool, device=DEVICE),\n",
    "                processor.tokenizer.pad_token_id,\n",
    "            )\n",
    "            .transpose(1, 0)\n",
    "        )\n",
    "        # This are the shifted labels\n",
    "        shifted_labels = torch.cat((labels[:, 1:], to_concat), dim=1)\n",
    "        # Create the Attention Mask for the decoder\n",
    "        # shifted_mask = torch.ones_like(shifted_labels, device=\"cuda\")\n",
    "        # The attention is: 0 for pad token (or tokens to ignore), 1 for the other values\n",
    "        shifted_mask = torch.where(\n",
    "            shifted_labels == processor.tokenizer.pad_token_id, 0, 1\n",
    "        ).to(DEVICE)\n",
    "        # Call the forward method to get the logits\n",
    "        # print(chunk_images.shape, input_labels.shape, shifted_mask.shape)\n",
    "        logits = trocr_model.forward(chunk_images, input_labels, shifted_mask).logits\n",
    "        # print(logits)\n",
    "        # Resize or rearrange the logits to match the VOCAB_SIZE dim (and embedding size of the model)\n",
    "        loss = ls_fn(\n",
    "            logits.contiguous().view(-1, trocr_model.config.decoder.vocab_size),\n",
    "            shifted_labels.contiguous().view(-1),\n",
    "        )\n",
    "        # Get the loss item\n",
    "        loss_item = loss.item()\n",
    "        # Reset the grad\n",
    "        optim.zero_grad()\n",
    "        # Derivative to update the weights\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        losses += loss_item\n",
    "    return losses / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_step(trocr_model: VisionEncoderDecoderModel, eval_dataset, ls_fn):\n",
    "    eval_loss = 0\n",
    "    trocr_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for eval_chunk in tqdm(eval_dataset):\n",
    "            chunk_images = [x[\"image\"] for x in eval_chunk]\n",
    "            labels = [x[\"label\"] for x in eval_chunk]\n",
    "            chunk_images = load_and_process_images(chunk_images, processor)\n",
    "            labels = processor.tokenizer(\n",
    "                labels, add_special_tokens=True, return_tensors=\"pt\", padding=True\n",
    "            )[\"input_ids\"].to(DEVICE)\n",
    "            input_labels = labels.clone()\n",
    "\n",
    "            input_labels = torch.where(\n",
    "                input_labels == processor.tokenizer.eos_token_id,\n",
    "                processor.tokenizer.pad_token_id,\n",
    "                input_labels,\n",
    "            )\n",
    "            to_concat = (\n",
    "                torch.empty((1, input_labels.shape[0]), dtype=torch.long, device=DEVICE)\n",
    "                .masked_fill(\n",
    "                    torch.ones(input_labels.shape[0], dtype=torch.bool, device=DEVICE),\n",
    "                    processor.tokenizer.pad_token_id,\n",
    "                )\n",
    "                .transpose(1, 0)\n",
    "            )\n",
    "            shifted_labels = torch.cat((labels[:, 1:], to_concat), dim=1)\n",
    "            shifted_mask = torch.where(\n",
    "                shifted_labels == processor.tokenizer.pad_token_id, 0, 1\n",
    "            ).to(DEVICE)\n",
    "            logits = trocr_model.forward(\n",
    "                chunk_images, input_labels, shifted_mask\n",
    "            ).logits\n",
    "            loss = ls_fn(\n",
    "                logits.contiguous().view(-1, trocr_model.config.decoder.vocab_size),\n",
    "                shifted_labels.contiguous().view(-1),\n",
    "            )\n",
    "            eval_loss += loss.item()\n",
    "    return eval_loss / len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cer_metric(trocr_model: VisionEncoderDecoderModel, trocr_processor: TrOCRProcessor, eval_dataset: list):\n",
    "    cer_value = 0\n",
    "    wer_value = 0\n",
    "    trocr_model.eval()\n",
    "    for eval_chunk in tqdm(eval_dataset):\n",
    "        chunk_images = [x[\"image\"] for x in eval_chunk]\n",
    "        tgt_text = [x[\"label\"] for x in eval_chunk]\n",
    "        chunk_images = load_and_process_images(chunk_images, processor)\n",
    "        model_output = trocr_model.generate(chunk_images)\n",
    "        model_texts = trocr_processor.batch_decode(model_output, skip_special_tokens=True)\n",
    "        \n",
    "        cer_val = fastwer.score(model_texts, tgt_text, char_level=True)\n",
    "        wer_val = fastwer.score(model_texts, tgt_text)\n",
    "        cer_value += cer_val\n",
    "        wer_value += wer_val\n",
    "    \n",
    "    return cer_value / len(eval_dataset), wer_value / len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = task.get_logger()\n",
    "model.train()\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    if epoch % EVAL_EVERY_N_EPOCHS == 0:\n",
    "        print(\"Starting Evaluation Step for EPOCH {}\".format(epoch))\n",
    "        eval_loss = evaluate_step(model, evaluation_dataset, loss_fn)\n",
    "        logger.report_scalar(\n",
    "            title=\"Loss\", series=\"Evaluation Loss\", value=eval_loss, iteration=epoch\n",
    "        )\n",
    "        print(\"Starting Evaluation CER/WER Step for EPOCH {}\".format(epoch))\n",
    "        cer_val, wer_val = compute_cer_metric(model, processor, evaluation_dataset)\n",
    "        logger.report_scalar(\n",
    "            title=\"Metrics\", series=\"CER\", value=cer_val, iteration=epoch\n",
    "        )\n",
    "        logger.report_scalar(\n",
    "            title=\"Metrics\", series=\"WER\", value=wer_val, iteration=epoch\n",
    "        )\n",
    "    print(\"Starting Training Step for EPOCH {}\".format(epoch))\n",
    "    train_loss = train_step(model, train_dataset, loss_fn, optimizer)\n",
    "    logger.report_scalar(\n",
    "        title=\"Loss\", series=\"Train Loss\", value=train_loss, iteration=epoch\n",
    "    )\n",
    "    if (\n",
    "        epoch > 0 and epoch % SAVE_CKP_EVERY_N_EPOCHS == 0\n",
    "    ):  # Save every N epochs, but not the 0 epoch\n",
    "        ckp_path = os.path.join(\n",
    "            CKP_PATH,\n",
    "            MODEL_CODENAME,\n",
    "            \"V_{}\".format(MODEL_VERSION),\n",
    "            \"Epoch_{}\".format(epoch),\n",
    "        )\n",
    "        model.save_pretrained(ckp_path, safe_serialization=True)\n",
    "        processor.save_pretrained(ckp_path)\n",
    "        print(\"Checkpoint Saved to {}\".format(ckp_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(FINAL_MODEL_PATH, exist_ok=True)\n",
    "final_ckp_file = os.path.join(\n",
    "    FINAL_MODEL_PATH, MODEL_CODENAME, \"V_{}_final\".format(MODEL_VERSION)\n",
    ")\n",
    "model.save_pretrained(final_ckp_file, safe_serialization=True)\n",
    "processor.save_pretrained(final_ckp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.flush()\n",
    "task.mark_completed()\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, the metrics were the following\n",
    "\n",
    "<img src=\"./images/trocr_metrics.png\" width=\"800\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyFormers + CUDA",
   "language": "python",
   "name": "dev-kernel-v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
