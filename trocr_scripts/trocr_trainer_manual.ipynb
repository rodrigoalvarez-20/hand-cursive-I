{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TROCR Pytorch Finetuning with CUSTOM DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook was used for finetuning Microsoft/trocr-large-stage1 base model (I dont use the Handwritten finetuned version to avoid language mistakes)\n",
    "\n",
    "I used the same dataset of the DETR project, but, insted of downloading in COCO JSON format, I downloaded in XML format and parse it with \"xml_workbench.ipynb\" lab\n",
    "\n",
    "The difference with the DETR dataset, this dataset contains all the original labels (~790 labels).\n",
    "\n",
    "The structure of the dataset is: \"image_path\" and \"label\" (in text)\n",
    "\n",
    "The reason for making the train cycle \"manually\" was for problems with GPU memory (Out of memory), to solve it, I implemented the cycle from scratch based on the CausalLLM Finetuning (SFTTrainer)\n",
    "\n",
    "Basically, I used the \"Right Shift\" technique.\n",
    "\n",
    "The input for the Encoder are the pixel values, the input for the Decoder is the target text including BOS token and excluding the EOS token, with padding.\n",
    "\n",
    "To ilustrate: \n",
    "\n",
    "* `This is the text`\n",
    "* `<bos> This is the text`\n",
    "* `This is the text <eos>`\n",
    "\n",
    "Author: Rodrigo Alvarez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ralvarez22/server/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/ralvarez22/server/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "from clearml import Task\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CLEARML_WEB_HOST=http://localhost:8080\n",
      "env: CLEARML_API_HOST=http://localhost:8008\n",
      "env: CLEARML_FILES_HOST=http://localhost:8081\n",
      "env: CLEARML_API_ACCESS_KEY=AEBY191O3R1U4SGBDPLA\n",
      "env: CLEARML_API_SECRET_KEY=OVvAzcKHtSfqP95jjMHgmgAvzDcSKIKRt5wv1hE1PerO5D3uiT\n",
      "env: CLEARML_LOG_MODEL=False\n"
     ]
    }
   ],
   "source": [
    "# This is in case to use ClearML (Local with docker) to Log the metrics\n",
    "%env CLEARML_WEB_HOST=http://localhost:8080\n",
    "%env CLEARML_API_HOST=http://localhost:8008\n",
    "%env CLEARML_FILES_HOST=http://localhost:8081\n",
    "%env CLEARML_API_ACCESS_KEY=AEBY191O3R1U4SGBDPLA\n",
    "%env CLEARML_API_SECRET_KEY=OVvAzcKHtSfqP95jjMHgmgAvzDcSKIKRt5wv1hE1PerO5D3uiT\n",
    "%env CLEARML_LOG_MODEL=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_CACHE = \"/home/ralvarez22/Documentos/llm_data/llm_cache\"\n",
    "TROCR_MODEL = \"/home/ralvarez22/Documentos/llm_data/llm_cache/models--microsoft--trocr-large-stage1/snapshots/3c8ead8dfda428d914334169380bb546f770a300\"\n",
    "\n",
    "DATASET_PATH = \"../hand-cursive-trocr\"\n",
    "\n",
    "METADATA_FILE = \"train_metadata.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ralvarez22/server/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"transformers_version\": \"4.46.3\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_hidden_size\": 1024,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": false,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at /home/ralvarez22/Documentos/llm_data/llm_cache/models--microsoft--trocr-large-stage1/snapshots/3c8ead8dfda428d914334169380bb546f770a300 and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Prepare the processor and the model\n",
    "processor = TrOCRProcessor.from_pretrained(TROCR_MODEL, cache_dir=HF_CACHE, device_map=\"cuda\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(TROCR_MODEL, cache_dir=HF_CACHE, device_map=\"cuda\")\n",
    "# In every tutorial I found, they dont modify the config of the processor and model\n",
    "# This configuration sets the special tokens for a valid Training and Inference use\n",
    "# Please make sure to set the decoder_start_token_id to the tokenizer bos_token_id\n",
    "# In some cases, the bos_token_id is the eos_token_id. This results in NO generation, because the end-of-sequence\n",
    "model.generation_config.decoder_start_token_id = processor.tokenizer.bos_token_id\n",
    "model.config.decoder.bos_token_id = processor.tokenizer.bos_token_id\n",
    "model.config.decoder.decoder_start_token_id = processor.tokenizer.bos_token_id\n",
    "model.config.decoder.eos_token_id = processor.tokenizer.eos_token_id\n",
    "model.config.decoder.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.encoder.bos_token_id = processor.tokenizer.bos_token_id\n",
    "model.config.encoder.decoder_start_token_id = processor.tokenizer.bos_token_id\n",
    "model.config.encoder.eos_token_id = processor.tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5 # Modify in case of CUDA OUT OF MEMORY\n",
    "MODEL_USED = \"Trocr Large Stage 1\" # Name of the model used, this for logs\n",
    "CKP_PATH = \"../finetuned/trocr\"\n",
    "FINAL_MODEL_PATH = \"../finetuned/trocr\"\n",
    "MODEL_CODENAME = \"Terminus\" # Model Codename versioning\n",
    "MODEL_VERSION = 1\n",
    "SAVE_CKP_EVERY = 20\n",
    "MAX_ITEMS = -1\n",
    "EPOCHS = 5 # I use this value because it was only a Proof of concept test. With more Epochs, the accurancy (in theory) should be better\n",
    "LR = 1e-5 # All the tutorials recommend 4e-5 or 5e-5, but, I couldn't get a good model, the model stopped learning at the epoch 20 or 25 and the Loss Graph begun to raise instead of go down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to manually chunk the data\n",
    "def divide_chunks(l, n):\n",
    "    # looping till length l \n",
    "    for i in range(0, len(l), n):  \n",
    "        yield l[i:i + n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metadata file\n",
    "dataset_metadata = json.load(open(os.path.join(DATASET_PATH, METADATA_FILE), \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAX_ITEMS > 0:\n",
    "    dataset_metadata = dataset_metadata[:MAX_ITEMS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the chunks\n",
    "chunked_dataset = list(divide_chunks(dataset_metadata, BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_info = {\n",
    "    \"type\": \"TROCR Cursive Handwritten\",\n",
    "    \"codename\": MODEL_CODENAME,\n",
    "    \"version\": MODEL_VERSION,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"learning_rate\": LR,\n",
    "    \"dataset\": \"Handwritten App V1\",\n",
    "    \"model\": MODEL_USED\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 609.169408\n",
      "Trainable params: 609.169408 M\n"
     ]
    }
   ],
   "source": [
    "trocr_total_params = sum(p.numel() for p in model.parameters())\n",
    "trocr_train_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total params: {}\\nTrainable params: {} M\".format(trocr_total_params / 1e6, trocr_train_params/ 1e6))\n",
    "log_info[\"total_params\"] = trocr_total_params\n",
    "log_info[\"trainable_params\"] = trocr_train_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=18714187cb0b475282f708791d8c99a3\n",
      "2025-01-17 17:49:18,733 - clearml.Task - INFO - Storing jupyter notebook directly as code\n",
      "2025-01-17 17:49:18,744 - clearml.Repository Detection - WARNING - Can't get url information for git repo in /home/ralvarez22/Documentos/trocr_hand/hand-cursive-I/trocr_scripts\n",
      "2025-01-17 17:49:18,751 - clearml.Repository Detection - WARNING - Can't get branch information for git repo in /home/ralvarez22/Documentos/trocr_hand/hand-cursive-I/trocr_scripts\n",
      "2025-01-17 17:49:18,758 - clearml.Repository Detection - WARNING - Can't get commit information for git repo in /home/ralvarez22/Documentos/trocr_hand/hand-cursive-I/trocr_scripts\n",
      "2025-01-17 17:49:18,831 - clearml.Repository Detection - WARNING - Can't get diff information for git repo in /home/ralvarez22/Documentos/trocr_hand/hand-cursive-I/trocr_scripts\n",
      "CLEARML-SERVER new package available: UPGRADE to v2.0.0 is recommended!\n",
      "Release Notes:\n",
      "### Breaking Changes\n",
      "\n",
      "MongoDB major version was upgraded from v5.x to 6.x.\n",
      "Please note that if your current ClearML Server version is smaller than v1.17 (where MongoDB v5.x was first used), you'll need to first upgrade to ClearML Server v1.17.\n",
      "#### Upgrading to ClearML Server v1.17 from a previous version\n",
      "- If using docker-compose,  use the following docker-compose files:\n",
      "  * [docker-compose file](https://github.com/allegroai/clearml-server/blob/2976ce69cc91550a3614996e8a8d8cd799af2efd/upgrade/1_17_to_2_0/docker-compose.yml)\n",
      "  * [docker-compose file foe Windows](https://github.com/allegroai/clearml-server/blob/2976ce69cc91550a3614996e8a8d8cd799af2efd/upgrade/1_17_to_2_0/docker-compose-win10.yml)\n",
      "\n",
      "### New Features\n",
      "\n",
      "- New look and feel: Full light/dark themes ([clearml #1297](https://github.com/allegroai/clearml/issues/1297))\n",
      "- New UI task creation options\n",
      "  - Support bash as well as python scripts\n",
      "  - Support file upload\n",
      "- New UI setting for configuring cloud storage credentials with which ClearML can clean up cloud storage artifacts on task deletion. \n",
      "- Add UI scalar plots presentation of plots in sections grouped by metrics.\n",
      "- Add UI Batch export plot embed codes for all metric plots in a single click.\n",
      "- Add UI pipeline presentation of steps grouped into stages\n",
      "\n",
      "### Bug Fixes\n",
      "- Fix UI Model Endpoint's Number of Requests plot sometimes displays incorrect data\n",
      "- Fix UI datasets page does not filter according to project when dataset is running \n",
      "- Fix UI task scalar legend does not change colors when smoothing is enabled \n",
      "- Fix queue list in UI Workers and Queues page does not alphabetically sort by queue display name \n",
      "- Fix queue display name is not searchable in UI Task Creation modal's queue field\n",
      "\n",
      "ClearML results page: http://localhost:8080/projects/5d5fc54109424b7f9b474ecbb76288e5/experiments/18714187cb0b475282f708791d8c99a3/output/log\n"
     ]
    }
   ],
   "source": [
    "tsk_name = \"{}_V{}\".format(MODEL_CODENAME, str(MODEL_VERSION))\n",
    "task = Task.init(task_name=tsk_name, project_name=\"HandCursive-I\")\n",
    "task.set_parameters(log_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the Loss Function (CrossEntropy) and the Optimizer (AdamW)\n",
    "# I set the ignore_index to the tokenizer pad token to avoid bad calculations\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=processor.tokenizer.pad_token_id)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving chekpoints to ../finetuned/trocr/Terminus/V_1\n"
     ]
    }
   ],
   "source": [
    "model_name = \"{}/V_{}\".format(MODEL_CODENAME, MODEL_VERSION)\n",
    "epochs_path = os.path.join(CKP_PATH, model_name)\n",
    "print(\"Saving chekpoints to {}\".format(epochs_path))\n",
    "os.makedirs(epochs_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliar function to open the image and load the pixel values\n",
    "def load_and_process_images(images_chunk, troc_proc):\n",
    "    proc_chunk = []\n",
    "    for x in images_chunk:\n",
    "        proc_chunk.append(\n",
    "            troc_proc(\n",
    "                Image.open(os.path.join(DATASET_PATH, x)).convert(\"RGB\"),\n",
    "                return_tensors=\"pt\",\n",
    "            ).pixel_values.to(\"cuda\")\n",
    "        )\n",
    "    # Use squeeze to eliminate the single array dimension of every item\n",
    "    return torch.stack(proc_chunk, 0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"training step\" function\n",
    "def train_epoch(trocr_model: VisionEncoderDecoderModel, dataset, ls_fn, optim):\n",
    "    losses = 0  # Accumulation of loss for every epoch\n",
    "    for chunk in dataset:\n",
    "        chunk_images = [x[\"image\"] for x in chunk]\n",
    "        labels = [x[\"label\"] for x in chunk]\n",
    "        chunk_images = load_and_process_images(\n",
    "            chunk_images, processor\n",
    "        )  # Process the batch images and get the batch pixels\n",
    "        # Tokenize the labels\n",
    "        labels = processor.tokenizer(\n",
    "            labels, add_special_tokens=True, return_tensors=\"pt\", padding=True\n",
    "        )[\"input_ids\"].to(\"cuda\")\n",
    "        # Clone the labels to avoid modifications in the original tensor\n",
    "        input_labels = labels.clone()\n",
    "        # Convert the EOS token to a padding token\n",
    "        input_labels = torch.where(\n",
    "            input_labels == processor.tokenizer.eos_token_id,\n",
    "            processor.tokenizer.pad_token_id,\n",
    "            input_labels,\n",
    "        )\n",
    "        # Because I shifted 1 item to the right, I need to add an additional token to preserve the dimensions\n",
    "        to_concat = (\n",
    "            torch.empty((1, input_labels.shape[0]), dtype=torch.long, device=\"cuda\")\n",
    "            .masked_fill(\n",
    "                torch.ones(input_labels.shape[0], dtype=torch.bool, device=\"cuda\"),\n",
    "                processor.tokenizer.pad_token_id,\n",
    "            )\n",
    "            .transpose(1, 0)\n",
    "        )\n",
    "        # This are the shifted labels\n",
    "        shifted_labels = torch.cat((labels[:, 1:], to_concat), dim=1)\n",
    "        # Create the Attention Mask for the decoder\n",
    "        # shifted_mask = torch.ones_like(shifted_labels, device=\"cuda\")\n",
    "        # The attention is: 0 for pad token (or tokens to ignore), 1 for the other values\n",
    "        shifted_mask = torch.where(\n",
    "            shifted_labels == processor.tokenizer.pad_token_id, 0, 1\n",
    "        ).to(\"cuda\")\n",
    "        # Call the forward method to get the logits\n",
    "        # print(chunk_images.shape, input_labels.shape, shifted_mask.shape)\n",
    "        logits = trocr_model.forward(chunk_images, input_labels, shifted_mask).logits\n",
    "        # print(logits)\n",
    "        # Resize or rearrange the logits to match the VOCAB_SIZE dim (and embedding size of the model)\n",
    "        loss = ls_fn(\n",
    "            logits.contiguous().view(-1, trocr_model.config.decoder.vocab_size),\n",
    "            shifted_labels.contiguous().view(-1),\n",
    "        )\n",
    "        # Get the loss item\n",
    "        loss_item = loss.item()\n",
    "        # Reset the grad\n",
    "        optim.zero_grad()\n",
    "        # Derivative to update the weights\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        losses += loss_item\n",
    "    return losses / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e6f47b940a44e8a5418e2d4bd89113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEARML-SERVER new package available: UPGRADE to v2.0.0 is recommended!\n",
      "Release Notes:\n",
      "### Breaking Changes\n",
      "\n",
      "MongoDB major version was upgraded from v5.x to 6.x.\n",
      "Please note that if your current ClearML Server version is smaller than v1.17 (where MongoDB v5.x was first used), you'll need to first upgrade to ClearML Server v1.17.\n",
      "#### Upgrading to ClearML Server v1.17 from a previous version\n",
      "- If using docker-compose,  use the following docker-compose files:\n",
      "  * [docker-compose file](https://github.com/allegroai/clearml-server/blob/2976ce69cc91550a3614996e8a8d8cd799af2efd/upgrade/1_17_to_2_0/docker-compose.yml)\n",
      "  * [docker-compose file foe Windows](https://github.com/allegroai/clearml-server/blob/2976ce69cc91550a3614996e8a8d8cd799af2efd/upgrade/1_17_to_2_0/docker-compose-win10.yml)\n",
      "\n",
      "### New Features\n",
      "\n",
      "- New look and feel: Full light/dark themes ([clearml #1297](https://github.com/allegroai/clearml/issues/1297))\n",
      "- New UI task creation options\n",
      "  - Support bash as well as python scripts\n",
      "  - Support file upload\n",
      "- New UI setting for configuring cloud storage credentials with which ClearML can clean up cloud storage artifacts on task deletion. \n",
      "- Add UI scalar plots presentation of plots in sections grouped by metrics.\n",
      "- Add UI Batch export plot embed codes for all metric plots in a single click.\n",
      "- Add UI pipeline presentation of steps grouped into stages\n",
      "\n",
      "### Bug Fixes\n",
      "- Fix UI Model Endpoint's Number of Requests plot sometimes displays incorrect data\n",
      "- Fix UI datasets page does not filter according to project when dataset is running \n",
      "- Fix UI task scalar legend does not change colors when smoothing is enabled \n",
      "- Fix queue list in UI Workers and Queues page does not alphabetically sort by queue display name \n",
      "- Fix queue display name is not searchable in UI Task Creation modal's queue field\n",
      "\n",
      "ClearML results page: http://localhost:8080/projects/5d5fc54109424b7f9b474ecbb76288e5/experiments/18714187cb0b475282f708791d8c99a3/output/log\n",
      "ClearML Monitor: Could not detect iteration reporting, falling back to iterations as seconds-from-start\n"
     ]
    }
   ],
   "source": [
    "logger = task.get_logger()\n",
    "model.train()\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    train_loss = train_epoch(model, chunked_dataset, loss_fn, optimizer)\n",
    "    if epoch > 0 and epoch % SAVE_CKP_EVERY == 0: # Save every N epochs, but not the 0 epoch\n",
    "        ckp_path = os.path.join(CKP_PATH, MODEL_CODENAME, \"V_{}\".format(MODEL_VERSION), \"Epoch_{}\".format(epoch))\n",
    "        model.save_pretrained(ckp_path, safe_serialization=True)\n",
    "        processor.save_pretrained(ckp_path)\n",
    "    #print(train_loss)\n",
    "    logger.report_scalar(title='Train Loss', series='Loss', value=train_loss, iteration=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(FINAL_MODEL_PATH, exist_ok=True)\n",
    "final_ckp_file = os.path.join(FINAL_MODEL_PATH, MODEL_CODENAME, \"V_{}_final\".format(MODEL_VERSION) )\n",
    "model.save_pretrained(final_ckp_file, safe_serialization=True)\n",
    "processor.save_pretrained(final_ckp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.flush()\n",
    "task.mark_completed()\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, the metrics were the following\n",
    "\n",
    "<img src=\"./images/trocr_metrics.png\" width=\"800\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyFormers + CUDA",
   "language": "python",
   "name": "dev-kernel-v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
