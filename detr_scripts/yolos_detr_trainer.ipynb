{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETR Pytorch Ligtning Finetuning with COCO-like dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook was used for finetuning facebook/detr-resnet-101 base model\n",
    "\n",
    "I used a COCO-like dataset in JSON format built with Roboflow Page with propietary images\n",
    "\n",
    "The labels for the dataset are 2:\n",
    "* words --> As supercategory\n",
    "* word --> As Category/Label\n",
    "\n",
    "The structure of the dataset is the following: \"image_path\", \"class_label_id\" and \"coords\" (wich is a 4 tuple with X, Y, Width, Height of the bounding box)\n",
    "\n",
    "I only used the label \"word\" in the entire dataset with the goal to only detect and generate the bounding boxes for Handwritten and Cursive text\n",
    "\n",
    "This script uses Pytorch with GPU support and Pytorch Lighning to use GPU Acceleration\n",
    "\n",
    "This code is mainly based on this [Roboflow Colab Tutorial](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-huggingface-detr-on-custom-dataset.ipynb)\n",
    "\n",
    "Author: Rodrigo Alvarez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision\n",
    "from transformers import YolosFeatureExtractor, YolosForObjectDetection\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import shutil\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET_DIR = \"../hand-cursive-detr\"\n",
    "TRAIN_ANNOTATION_FILE_NAME = \"_train_annotations_coco.json\"\n",
    "\n",
    "VAL_DATASET_DIR = \"../hand-cursive-detr\"\n",
    "VAL_ANNOTATION_FILE_NAME = \"_val_annotations_coco.json\"\n",
    "\n",
    "HF_CACHE = \"/home/ralvarez22/Documentos/llm_data/llm_cache\"  # I used HF cache dir to avoid the re-downloading of the model\n",
    "DEVICE = \"cuda\"  # CUDA, CPU, or Specific device (CUDA:0, CUDA:0)\n",
    "DETR_CHECKPOINT = \"hustvl/yolos-base\" #os.path.join(\n",
    "    #HF_CACHE,\n",
    "    #\"models--facebook--detr-resnet-101/snapshots/7d14702e444d98d0b1764824567fc2b45e1eb218\",\n",
    "#)  # If using HF_CACHE, please set the path to the \"snapshot\" dir of the model\n",
    "WEIGHT_DECAY = 1e-4\n",
    "CLIP_GRAD = 0.1  # 1e-4 #0.001\n",
    "BATCH_SIZE = 8\n",
    "ACC_BATCH =  BATCH_SIZE * 2\n",
    "MODEL_LR = 1e-4  # \n",
    "#BB_LR = 1e-5  # \n",
    "MAX_EPOCHS = 100  # Use >= 50 . But it stops learning near the step 70\n",
    "MAX_TRAIN_ITEMS = -1\n",
    "LAB_NAME = \"DETR_LAB\"  # For my logger\n",
    "EXPERIMENT_NAME = \"Roccia\"  # For logger and versioning\n",
    "EXPERIMENT_VERSION = 1  # Same\n",
    "CKP_PATH = \"../finetuned/yolos\"\n",
    "\n",
    "LOG_DIR = \"../logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(CKP_PATH, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom class for loading Coco datasets in JSON format\n",
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(\n",
    "        self, \n",
    "        image_directory_path: str,\n",
    "        annotations_filename: str,\n",
    "        image_processor, \n",
    "        train: bool = True\n",
    "    ):\n",
    "        annotation_file_path = os.path.join(image_directory_path, annotations_filename)\n",
    "        # Its not an error to pass the \"root_dataset_directory\" and \"full_annotation_file_path\", inside the code, they dont concatenate\n",
    "        super(CocoDetection, self).__init__(image_directory_path, annotation_file_path)\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        images, annotations = super(CocoDetection, self).__getitem__(idx)        \n",
    "        image_id = self.ids[idx]\n",
    "        annotations = {'image_id': image_id, 'annotations': annotations}\n",
    "        encoding = self.image_processor(images=images, annotations=annotations, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze()\n",
    "        target = encoding[\"labels\"][0]\n",
    "        return pixel_values, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ralvarez22/server/lib/python3.11/site-packages/transformers/models/yolos/feature_extraction_yolos.py:38: FutureWarning: The class YolosFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use YolosImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#First, load the image processor\n",
    "image_processor = YolosFeatureExtractor.from_pretrained(DETR_CHECKPOINT, cache_dir=HF_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.37s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.06s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset and create an \"id to label\" dictionary\n",
    "TRAIN_DATASET = CocoDetection(\n",
    "    image_directory_path=TRAIN_DATASET_DIR,\n",
    "    annotations_filename=TRAIN_ANNOTATION_FILE_NAME,\n",
    "    image_processor=image_processor,\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "VALIDATION_DATASET = CocoDetection(\n",
    "    image_directory_path=VAL_DATASET_DIR,\n",
    "    annotations_filename=VAL_ANNOTATION_FILE_NAME,\n",
    "    image_processor=image_processor,\n",
    "    train=False,\n",
    ")\n",
    "\n",
    "categories = TRAIN_DATASET.coco.cats\n",
    "id2label = {k: v[\"name\"] for k, v in categories.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 1117\n"
     ]
    }
   ],
   "source": [
    "if MAX_TRAIN_ITEMS > 0:\n",
    "    TRAIN_DATASET = TRAIN_DATASET[:MAX_TRAIN_ITEMS]\n",
    "print(\"Number of training examples:\", len(TRAIN_DATASET))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This function allow to process the data from a batch, the data must be pre-tokenized or pre-processed to be padded\n",
    "def collate_fn(batch):\n",
    "    # DETR authors employ various image sizes during training, making it not possible \n",
    "    # to directly batch together images. Hence they pad the images to the biggest \n",
    "    # resolution in a given batch, and create a corresponding binary pixel_mask \n",
    "    # which indicates which pixels are real/which are padding\n",
    "    pixel_values = [item[0] for item in batch]\n",
    "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\").to(DEVICE)\n",
    "    labels = [item[1] for item in batch]\n",
    "    return {\n",
    "        'pixel_values': encoding['pixel_values'],\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# Create the dataset loader, you can experiment with this\n",
    "# In case of CUDA OUT OF MEMORY error, reduce the batch size or, move the logic to process the image inside the collate_fn\n",
    "TRAIN_DATALOADER = DataLoader(dataset=TRAIN_DATASET, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "#VAL_DATALOADER = DataLoader(dataset=VALIDATION_DATASET, batch_size=BATCH_SIZE, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image n°361\n"
     ]
    }
   ],
   "source": [
    "# based on https://github.com/woctezuma/finetune-detr/blob/master/finetune_detr.ipynb\n",
    "image_ids = TRAIN_DATASET.coco.getImgIds()\n",
    "# let's pick a random image\n",
    "image_id = image_ids[np.random.randint(0, len(image_ids))]\n",
    "print('Image n°{}'.format(image_id))\n",
    "image = TRAIN_DATASET.coco.loadImgs(image_id)[0]\n",
    "image = Image.open(os.path.join(TRAIN_DATASET.root, image['file_name']))\n",
    "annotations = TRAIN_DATASET.coco.imgToAnns[image_id]\n",
    "draw = ImageDraw.Draw(image, \"RGBA\")\n",
    "cats = TRAIN_DATASET.coco.cats\n",
    "id2label = {k: v['name'] for k,v in cats.items()}\n",
    "for annotation in annotations:\n",
    "  box = annotation['bbox']\n",
    "  class_idx = annotation['category_id']\n",
    "  x,y,w,h = tuple(box)\n",
    "  draw.rectangle((x,y,x+w,y+h), outline='red', width=2)\n",
    "  draw.text((x, y), id2label[class_idx], fill='white')\n",
    "image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Python Lightning wrapper for the DETR Model to be able to use the Accelerator and Trainer\n",
    "class Detr(pl.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path,\n",
    "        lr,\n",
    "        weight_decay,\n",
    "        training_dataset,\n",
    "        validation_dataset=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = YolosForObjectDetection.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_path,\n",
    "            num_labels=len(id2label),\n",
    "            ignore_mismatched_sizes=True,\n",
    "            cache_dir=HF_CACHE,\n",
    "        )\n",
    "\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.training_dataset = training_dataset\n",
    "        self.validation_dataset = validation_dataset\n",
    "\n",
    "    def forward(self, pixel_values, pixel_mask):\n",
    "        return self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "    def common_step(self, batch, batch_idx):\n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "        outputs = self.model(\n",
    "            pixel_values=pixel_values, labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss_dict = outputs.loss_dict\n",
    "\n",
    "        return loss, loss_dict\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        # logs metrics for each training_step, and the average across the epoch\n",
    "        self.log(\"training_loss\", loss)\n",
    "        for k, v in loss_dict.items():\n",
    "            self.log(\"train_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        self.log(\"validation_loss\", loss)\n",
    "        for k, v in loss_dict.items():\n",
    "            self.log(\"validation_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(\n",
    "            self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.training_dataset\n",
    "\n",
    "    #def val_dataloader(self):\n",
    "    #    return self.validation_dataset if self.validation_dataset else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of YolosForObjectDetection were not initialized from the model checkpoint at hustvl/yolos-base and are newly initialized because the shapes did not match:\n",
      "- class_labels_classifier.layers.2.weight: found shape torch.Size([92, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "- class_labels_classifier.layers.2.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create a Pythorch Lightning model\n",
    "model = Detr(\n",
    "    DETR_CHECKPOINT,\n",
    "    lr=MODEL_LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    training_dataset=TRAIN_DATALOADER,\n",
    "    #validation_dataset=VAL_DATALOADER,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params for the logger\n",
    "hyperparams = {\n",
    "    \"model_type\": \"YOLO\",\n",
    "    \"model_name\": \"hustvl/yolos-base\",\n",
    "    \"codename\": EXPERIMENT_NAME,\n",
    "    \"version\": EXPERIMENT_VERSION,\n",
    "    \"model_learning_rate\": MODEL_LR,\n",
    "    \"epochs\": MAX_EPOCHS,\n",
    "    \"weight_decay\": WEIGHT_DECAY,\n",
    "    \"acc_grad_batches\": ACC_BATCH,\n",
    "    \"clip_grad\": CLIP_GRAD,\n",
    "    \"batch_size\": BATCH_SIZE\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 127.729927\n",
      "Trainable params: 127.729927 M\n"
     ]
    }
   ],
   "source": [
    "# To get an idea of the total of params and the trainable params\n",
    "detr_total_params = sum(p.numel() for p in model.parameters())\n",
    "detr_train_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total params: {}\\nTrainable params: {} M\".format(detr_total_params / 1e6, detr_train_params/ 1e6))\n",
    "hyperparams[\"total_params\"] = detr_total_params\n",
    "hyperparams[\"trainable_params\"] = detr_train_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join(LOG_DIR, EXPERIMENT_NAME, \"version_{}\".format(EXPERIMENT_VERSION))\n",
    "shutil.rmtree(log_path, ignore_errors=True)\n",
    "# I used Tensorboard Logger. If you too, please make sure to initiate the TB instance\n",
    "logger = pl.loggers.TensorBoardLogger(save_dir=LOG_DIR, version=EXPERIMENT_VERSION, name=EXPERIMENT_NAME)\n",
    "logger.log_hyperparams(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ralvarez22/server/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name  | Type                    | Params | Mode\n",
      "---------------------------------------------------------\n",
      "0 | model | YolosForObjectDetection | 127 M  | eval\n",
      "---------------------------------------------------------\n",
      "127 M     Trainable params\n",
      "0         Non-trainable params\n",
      "127 M     Total params\n",
      "510.920   Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "237       Modules in eval mode\n",
      "/home/ralvarez22/server/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aee2a79869449ab9b45c4c3ecdc3360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    }
   ],
   "source": [
    "# Create a PL Trainer.\n",
    "# I use 1 device (NVIDIA 4090 24GB). In the first attemp i wanted to use 2 GPUs (4090 & 3060) but the process finishes with error code.\n",
    "# Use the GPU accelerator to speed-up the training and avoid bad performance\n",
    "# accumulate_grad_batches is used to save memory, if you want to experiment, less acc value, more merory used (In theory)\n",
    "trainer = pl.Trainer(devices=1, accelerator=\"gpu\", max_epochs=MAX_EPOCHS, gradient_clip_val=CLIP_GRAD, accumulate_grad_batches=ACC_BATCH, log_every_n_steps=ACC_BATCH, check_val_every_n_epoch=ACC_BATCH, logger=logger)\n",
    "# Run the training Cycle and log the metrics\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, the metrics were the following\n",
    "\n",
    "![DETR Metrics](../metrics/detr_train_metrics.png \"Loss Metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../finetuned/yolos/Roccia/V_1/preprocessor_config.json']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the final model\n",
    "FINAL_CKP_PATH = os.path.join(CKP_PATH, EXPERIMENT_NAME, \"V_{}\".format(EXPERIMENT_VERSION))\n",
    "# By default it saves the Safetensors type\n",
    "model.model.save_pretrained(FINAL_CKP_PATH)\n",
    "image_processor.save_pretrained(FINAL_CKP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, you can push the model to HF Hub here, but is recommended to first test it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyFormers + CUDA",
   "language": "python",
   "name": "dev-kernel-v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
