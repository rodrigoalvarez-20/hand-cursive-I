{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETR Pytorch Ligtning Finetuning with COCO-like dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook was used for finetuning facebook/detr-resnet-101 base model\n",
    "\n",
    "I used a COCO-like dataset in JSON format built with Roboflow Page with propietary images\n",
    "\n",
    "The labels for the dataset are 2:\n",
    "* words --> As supercategory\n",
    "* word --> As Category/Label\n",
    "\n",
    "The structure of the dataset is the following: \"image_path\", \"class_label_id\" and \"coords\" (wich is a 4 tuple with X, Y, Width, Height of the bounding box)\n",
    "\n",
    "I only used the label \"word\" in the entire dataset with the goal to only detect and generate the bounding boxes for Handwritten and Cursive text\n",
    "\n",
    "This script uses Pytorch with GPU support and Pytorch Lighning to use GPU Acceleration\n",
    "\n",
    "This code is mainly based on this [Roboflow Colab Tutorial](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-huggingface-detr-on-custom-dataset.ipynb)\n",
    "\n",
    "Author: Rodrigo Alvarez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision\n",
    "from transformers import DetrForObjectDetection, DetrImageProcessor\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import shutil\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET_DIR = \"../hand-cursive-detr\"\n",
    "TRAIN_ANNOTATION_FILE_NAME = \"_train_annotations_coco.json\"\n",
    "\n",
    "VAL_DATASET_DIR = \"../hand-cursive-detr\"\n",
    "VAL_ANNOTATION_FILE_NAME = \"_val_annotations_coco.json\"\n",
    "\n",
    "HF_CACHE = \"/home/ralvarez22/Documentos/llm_data/llm_cache\"  # I used HF cache dir to avoid the re-downloading of the model\n",
    "DEVICE = \"cuda\"  # CUDA, CPU, or Specific device (CUDA:0, CUDA:0)\n",
    "DETR_CHECKPOINT = os.path.join(\n",
    "    HF_CACHE,\n",
    "    \"models--facebook--detr-resnet-101/snapshots/7d14702e444d98d0b1764824567fc2b45e1eb218\",\n",
    ")  # If using HF_CACHE, please set the path to the \"snapshot\" dir of the model\n",
    "WEIGHT_DECAY = 1e-4\n",
    "CLIP_GRAD = 0.1  # 1e-4 #0.001\n",
    "BATCH_SIZE = 15\n",
    "ACC_BATCH =  BATCH_SIZE * 2\n",
    "MODEL_LR = 1e-4  # \n",
    "BB_LR = 1e-5  # \n",
    "MAX_EPOCHS = 60  # Use >= 50 . But it stops learning near the step 70\n",
    "MAX_TRAIN_ITEMS = -1\n",
    "LAB_NAME = \"DETR_LAB\"  # For my logger\n",
    "EXPERIMENT_NAME = \"Nous\"  # For logger and versioning\n",
    "EXPERIMENT_VERSION = 4  # Same\n",
    "CKP_PATH = \"../finetuned/detr\"\n",
    "\n",
    "LOG_DIR = \"../tb_logs_detr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(CKP_PATH, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom class for loading Coco datasets in JSON format\n",
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(\n",
    "        self, \n",
    "        image_directory_path: str,\n",
    "        annotations_filename: str,\n",
    "        image_processor, \n",
    "        train: bool = True\n",
    "    ):\n",
    "        annotation_file_path = os.path.join(image_directory_path, annotations_filename)\n",
    "        # Its not an error to pass the \"root_dataset_directory\" and \"full_annotation_file_path\", inside the code, they dont concatenate\n",
    "        super(CocoDetection, self).__init__(image_directory_path, annotation_file_path)\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        images, annotations = super(CocoDetection, self).__getitem__(idx)        \n",
    "        image_id = self.ids[idx]\n",
    "        annotations = {'image_id': image_id, 'annotations': annotations}\n",
    "        encoding = self.image_processor(images=images, annotations=annotations, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze()\n",
    "        target = encoding[\"labels\"][0]\n",
    "        return pixel_values, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#First, load the image processor\n",
    "image_processor = DetrImageProcessor.from_pretrained(DETR_CHECKPOINT, cache_dir=HF_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and create an \"id to label\" dictionary\n",
    "TRAIN_DATASET = CocoDetection(\n",
    "    image_directory_path=TRAIN_DATASET_DIR,\n",
    "    annotations_filename=TRAIN_ANNOTATION_FILE_NAME,\n",
    "    image_processor=image_processor,\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "VALIDATION_DATASET = CocoDetection(\n",
    "    image_directory_path=VAL_DATASET_DIR,\n",
    "    annotations_filename=VAL_ANNOTATION_FILE_NAME,\n",
    "    image_processor=image_processor,\n",
    "    train=False,\n",
    ")\n",
    "\n",
    "categories = TRAIN_DATASET.coco.cats\n",
    "id2label = {k: v[\"name\"] for k, v in categories.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAX_TRAIN_ITEMS > 0:\n",
    "    TRAIN_DATASET = TRAIN_DATASET[:MAX_TRAIN_ITEMS]\n",
    "print(\"Number of training examples:\", len(TRAIN_DATASET))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This function allow to process the data from a batch, the data must be pre-tokenized or pre-processed to be padded\n",
    "def collate_fn(batch):\n",
    "    # DETR authors employ various image sizes during training, making it not possible \n",
    "    # to directly batch together images. Hence they pad the images to the biggest \n",
    "    # resolution in a given batch, and create a corresponding binary pixel_mask \n",
    "    # which indicates which pixels are real/which are padding\n",
    "    pixel_values = [item[0] for item in batch]\n",
    "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\").to(DEVICE)\n",
    "    labels = [item[1] for item in batch]\n",
    "    return {\n",
    "        'pixel_values': encoding['pixel_values'],\n",
    "        'pixel_mask': encoding['pixel_mask'],\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# Create the dataset loader, you can experiment with this\n",
    "# In case of CUDA OUT OF MEMORY error, reduce the batch size or, move the logic to process the image inside the collate_fn\n",
    "TRAIN_DATALOADER = DataLoader(dataset=TRAIN_DATASET, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "VAL_DATALOADER = DataLoader(dataset=VALIDATION_DATASET, batch_size=BATCH_SIZE, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://github.com/woctezuma/finetune-detr/blob/master/finetune_detr.ipynb\n",
    "image_ids = TRAIN_DATASET.coco.getImgIds()\n",
    "# let's pick a random image\n",
    "image_id = image_ids[np.random.randint(0, len(image_ids))]\n",
    "print('Image nÂ°{}'.format(image_id))\n",
    "image = TRAIN_DATASET.coco.loadImgs(image_id)[0]\n",
    "image = Image.open(os.path.join(TRAIN_DATASET.root, image['file_name']))\n",
    "annotations = TRAIN_DATASET.coco.imgToAnns[image_id]\n",
    "draw = ImageDraw.Draw(image, \"RGBA\")\n",
    "cats = TRAIN_DATASET.coco.cats\n",
    "id2label = {k: v['name'] for k,v in cats.items()}\n",
    "for annotation in annotations:\n",
    "  box = annotation['bbox']\n",
    "  class_idx = annotation['category_id']\n",
    "  x,y,w,h = tuple(box)\n",
    "  draw.rectangle((x,y,x+w,y+h), outline='red', width=2)\n",
    "  draw.text((x, y), id2label[class_idx], fill='white')\n",
    "image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Python Lightning wrapper for the DETR Model to be able to use the Accelerator and Trainer\n",
    "class Detr(pl.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path,\n",
    "        lr,\n",
    "        lr_backbone,\n",
    "        weight_decay,\n",
    "        training_dataset,\n",
    "        validation_dataset=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = DetrForObjectDetection.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_path,\n",
    "            num_labels=len(id2label),\n",
    "            ignore_mismatched_sizes=True,\n",
    "            cache_dir=HF_CACHE,\n",
    "        )\n",
    "\n",
    "        self.lr = lr\n",
    "        self.lr_backbone = lr_backbone\n",
    "        self.weight_decay = weight_decay\n",
    "        self.training_dataset = training_dataset\n",
    "        self.validation_dataset = validation_dataset\n",
    "\n",
    "    def forward(self, pixel_values, pixel_mask):\n",
    "        return self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "    def common_step(self, batch, batch_idx):\n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        pixel_mask = batch[\"pixel_mask\"]\n",
    "        labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "        outputs = self.model(\n",
    "            pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss_dict = outputs.loss_dict\n",
    "\n",
    "        return loss, loss_dict\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        # logs metrics for each training_step, and the average across the epoch\n",
    "        self.log(\"training_loss\", loss)\n",
    "        for k, v in loss_dict.items():\n",
    "            self.log(\"train_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        self.log(\"validation_loss\", loss)\n",
    "        for k, v in loss_dict.items():\n",
    "            self.log(\"validation_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param_dicts = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.named_parameters()\n",
    "                    if \"backbone\" not in n and p.requires_grad\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.named_parameters()\n",
    "                    if \"backbone\" in n and p.requires_grad\n",
    "                ],\n",
    "                \"lr\": self.lr_backbone,\n",
    "            },\n",
    "        ]\n",
    "        # Feel free to modify the optimizer, I used this because the good performance in other projects and is the frequenly used in transformers models\n",
    "        return torch.optim.AdamW(\n",
    "            param_dicts, lr=self.lr, weight_decay=self.weight_decay\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.training_dataset\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.validation_dataset if self.validation_dataset else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pythorch Lightning model\n",
    "model = Detr(\n",
    "    DETR_CHECKPOINT,\n",
    "    lr=MODEL_LR,\n",
    "    lr_backbone=BB_LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    training_dataset=TRAIN_DATALOADER,\n",
    "    validation_dataset=VAL_DATALOADER,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params for the logger\n",
    "hyperparams = {\n",
    "    \"model_type\": \"DETR\",\n",
    "    \"model_name\": \"detr-resnet-101\",\n",
    "    \"codename\": EXPERIMENT_NAME,\n",
    "    \"version\": EXPERIMENT_VERSION,\n",
    "    \"model_learning_rate\": MODEL_LR,\n",
    "    \"backbone_learning_rate\": BB_LR,\n",
    "    \"epochs\": MAX_EPOCHS,\n",
    "    \"weight_decay\": WEIGHT_DECAY,\n",
    "    \"acc_grad_batches\": ACC_BATCH,\n",
    "    \"clip_grad\": CLIP_GRAD,\n",
    "    \"batch_size\": BATCH_SIZE\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get an idea of the total of params and the trainable params\n",
    "detr_total_params = sum(p.numel() for p in model.parameters())\n",
    "detr_train_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total params: {}\\nTrainable params: {} M\".format(detr_total_params / 1e6, detr_train_params/ 1e6))\n",
    "hyperparams[\"total_params\"] = detr_total_params\n",
    "hyperparams[\"trainable_params\"] = detr_train_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join(LOG_DIR, EXPERIMENT_NAME, \"version_{}\".format(EXPERIMENT_VERSION))\n",
    "shutil.rmtree(log_path, ignore_errors=True)\n",
    "# I used Tensorboard Logger. If you too, please make sure to initiate the TB instance\n",
    "logger = pl.loggers.TensorBoardLogger(save_dir=LOG_DIR, version=EXPERIMENT_VERSION, name=EXPERIMENT_NAME)\n",
    "logger.log_hyperparams(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PL Trainer.\n",
    "# I use 1 device (NVIDIA 4090 24GB). In the first attemp i wanted to use 2 GPUs (4090 & 3060) but the process finishes with error code.\n",
    "# Use the GPU accelerator to speed-up the training and avoid bad performance\n",
    "# accumulate_grad_batches is used to save memory, if you want to experiment, less acc value, more merory used (In theory)\n",
    "trainer = pl.Trainer(devices=1, accelerator=\"gpu\", max_epochs=MAX_EPOCHS, gradient_clip_val=CLIP_GRAD, accumulate_grad_batches=ACC_BATCH, log_every_n_steps=ACC_BATCH, check_val_every_n_epoch=ACC_BATCH, logger=logger)\n",
    "# Run the training Cycle and log the metrics\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, the metrics were the following\n",
    "\n",
    "![DETR Metrics](../metrics/detr_train_metrics.png \"Loss Metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "FINAL_CKP_PATH = os.path.join(CKP_PATH, EXPERIMENT_NAME, \"V_{}\".format(EXPERIMENT_VERSION))\n",
    "# By default it saves the Safetensors type\n",
    "model.model.save_pretrained(FINAL_CKP_PATH)\n",
    "image_processor.save_pretrained(FINAL_CKP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, you can push the model to HF Hub here, but is recommended to first test it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyFormers + CUDA",
   "language": "python",
   "name": "dev-kernel-v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
