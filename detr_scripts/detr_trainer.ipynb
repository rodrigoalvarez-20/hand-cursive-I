{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETR Pytorch Ligtning Finetuning with COCO-like dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook was used for finetuning facebook/detr-resnet-101 base model\n",
    "\n",
    "I used a COCO-like dataset in JSON format built with Roboflow Page with propietary images\n",
    "\n",
    "The labels for the dataset are 2:\n",
    "* words --> As supercategory\n",
    "* word --> As Category/Label\n",
    "\n",
    "The structure of the dataset is the following: \"image_path\", \"class_label_id\" and \"coords\" (wich is a 4 tuple with X, Y, Width, Height of the bounding box)\n",
    "\n",
    "I only used the label \"word\" in the entire dataset with the goal to only detect and generate the bounding boxes for Handwritten and Cursive text\n",
    "\n",
    "This script uses Pytorch with GPU support and Pytorch Lighning to use GPU Acceleration\n",
    "\n",
    "This code is mainly based on this [Roboflow Colab Tutorial](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-huggingface-detr-on-custom-dataset.ipynb)\n",
    "\n",
    "Author: Rodrigo Alvarez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ralvarez22/server/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/ralvarez22/server/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torchvision\n",
    "from transformers import DetrForObjectDetection, DetrImageProcessor\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import shutil\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET_DIR = \"../hand-cursive-detr\"\n",
    "TRAIN_ANNOTATION_FILE_NAME = \"_train_annotations_coco.json\"\n",
    "\n",
    "VAL_DATASET_DIR = \"../hand-cursive-detr\"\n",
    "VAL_ANNOTATION_FILE_NAME = \"_val_annotations_coco.json\"\n",
    "\n",
    "HF_CACHE = \"/home/ralvarez22/Documentos/llm_data/llm_cache\"  # I used HF cache dir to avoid the re-downloading of the model\n",
    "DEVICE = \"cuda\"  # CUDA, CPU, or Specific device (CUDA:0, CUDA:0)\n",
    "DETR_CHECKPOINT = os.path.join(\n",
    "    HF_CACHE,\n",
    "    \"models--facebook--detr-resnet-101/snapshots/7d14702e444d98d0b1764824567fc2b45e1eb218\",\n",
    ")  # If using HF_CACHE, please set the path to the \"snapshot\" dir of the model\n",
    "WEIGHT_DECAY = 1e-4\n",
    "CLIP_GRAD = 0.1  # 1e-4 #0.001\n",
    "BATCH_SIZE = 15\n",
    "ACC_BATCH =  BATCH_SIZE * 2\n",
    "MODEL_LR = 1e-4  # \n",
    "BB_LR = 1e-5  # \n",
    "MAX_EPOCHS = 60  # Use >= 50 . But it stops learning near the step 70\n",
    "MAX_TRAIN_ITEMS = -1\n",
    "LAB_NAME = \"DETR_LAB\"  # For my logger\n",
    "EXPERIMENT_NAME = \"Nous\"  # For logger and versioning\n",
    "EXPERIMENT_VERSION = 3  # Same\n",
    "CKP_PATH = \"../finetuned/detr\"\n",
    "\n",
    "LOG_DIR = \"../tb_logs_detr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(CKP_PATH, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom class for loading Coco datasets in JSON format\n",
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(\n",
    "        self, \n",
    "        image_directory_path: str,\n",
    "        annotations_filename: str,\n",
    "        image_processor, \n",
    "        train: bool = True\n",
    "    ):\n",
    "        annotation_file_path = os.path.join(image_directory_path, annotations_filename)\n",
    "        # Its not an error to pass the \"root_dataset_directory\" and \"full_annotation_file_path\", inside the code, they dont concatenate\n",
    "        super(CocoDetection, self).__init__(image_directory_path, annotation_file_path)\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        images, annotations = super(CocoDetection, self).__getitem__(idx)        \n",
    "        image_id = self.ids[idx]\n",
    "        annotations = {'image_id': image_id, 'annotations': annotations}\n",
    "        encoding = self.image_processor(images=images, annotations=annotations, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze()\n",
    "        target = encoding[\"labels\"][0]\n",
    "        return pixel_values, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#First, load the image processor\n",
    "image_processor = DetrImageProcessor.from_pretrained(DETR_CHECKPOINT, cache_dir=HF_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.39s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.30s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset and create an \"id to label\" dictionary\n",
    "TRAIN_DATASET = CocoDetection(\n",
    "    image_directory_path=TRAIN_DATASET_DIR,\n",
    "    annotations_filename=TRAIN_ANNOTATION_FILE_NAME,\n",
    "    image_processor=image_processor,\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "VALIDATION_DATASET = CocoDetection(\n",
    "    image_directory_path=VAL_DATASET_DIR,\n",
    "    annotations_filename=VAL_ANNOTATION_FILE_NAME,\n",
    "    image_processor=image_processor,\n",
    "    train=False,\n",
    ")\n",
    "\n",
    "categories = TRAIN_DATASET.coco.cats\n",
    "id2label = {k: v[\"name\"] for k, v in categories.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 1117\n"
     ]
    }
   ],
   "source": [
    "if MAX_TRAIN_ITEMS > 0:\n",
    "    TRAIN_DATASET = TRAIN_DATASET[:MAX_TRAIN_ITEMS]\n",
    "print(\"Number of training examples:\", len(TRAIN_DATASET))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This function allow to process the data from a batch, the data must be pre-tokenized or pre-processed to be padded\n",
    "def collate_fn(batch):\n",
    "    # DETR authors employ various image sizes during training, making it not possible \n",
    "    # to directly batch together images. Hence they pad the images to the biggest \n",
    "    # resolution in a given batch, and create a corresponding binary pixel_mask \n",
    "    # which indicates which pixels are real/which are padding\n",
    "    pixel_values = [item[0] for item in batch]\n",
    "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\").to(DEVICE)\n",
    "    labels = [item[1] for item in batch]\n",
    "    return {\n",
    "        'pixel_values': encoding['pixel_values'],\n",
    "        'pixel_mask': encoding['pixel_mask'],\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# Create the dataset loader, you can experiment with this\n",
    "# In case of CUDA OUT OF MEMORY error, reduce the batch size or, move the logic to process the image inside the collate_fn\n",
    "TRAIN_DATALOADER = DataLoader(dataset=TRAIN_DATASET, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "VAL_DATALOADER = DataLoader(dataset=VALIDATION_DATASET, batch_size=BATCH_SIZE, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image nÂ°839\n"
     ]
    }
   ],
   "source": [
    "# based on https://github.com/woctezuma/finetune-detr/blob/master/finetune_detr.ipynb\n",
    "image_ids = TRAIN_DATASET.coco.getImgIds()\n",
    "# let's pick a random image\n",
    "image_id = image_ids[np.random.randint(0, len(image_ids))]\n",
    "print('Image nÂ°{}'.format(image_id))\n",
    "image = TRAIN_DATASET.coco.loadImgs(image_id)[0]\n",
    "image = Image.open(os.path.join(TRAIN_DATASET.root, image['file_name']))\n",
    "annotations = TRAIN_DATASET.coco.imgToAnns[image_id]\n",
    "draw = ImageDraw.Draw(image, \"RGBA\")\n",
    "cats = TRAIN_DATASET.coco.cats\n",
    "id2label = {k: v['name'] for k,v in cats.items()}\n",
    "for annotation in annotations:\n",
    "  box = annotation['bbox']\n",
    "  class_idx = annotation['category_id']\n",
    "  x,y,w,h = tuple(box)\n",
    "  draw.rectangle((x,y,x+w,y+h), outline='red', width=2)\n",
    "  draw.text((x, y), id2label[class_idx], fill='white')\n",
    "image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Python Lightning wrapper for the DETR Model to be able to use the Accelerator and Trainer\n",
    "class Detr(pl.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path,\n",
    "        lr,\n",
    "        lr_backbone,\n",
    "        weight_decay,\n",
    "        training_dataset,\n",
    "        validation_dataset=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = DetrForObjectDetection.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_path,\n",
    "            num_labels=len(id2label),\n",
    "            ignore_mismatched_sizes=True,\n",
    "            cache_dir=HF_CACHE,\n",
    "        )\n",
    "\n",
    "        self.lr = lr\n",
    "        self.lr_backbone = lr_backbone\n",
    "        self.weight_decay = weight_decay\n",
    "        self.training_dataset = training_dataset\n",
    "        self.validation_dataset = validation_dataset\n",
    "\n",
    "    def forward(self, pixel_values, pixel_mask):\n",
    "        return self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "    def common_step(self, batch, batch_idx):\n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        pixel_mask = batch[\"pixel_mask\"]\n",
    "        labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "        outputs = self.model(\n",
    "            pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss_dict = outputs.loss_dict\n",
    "\n",
    "        return loss, loss_dict\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        # logs metrics for each training_step, and the average across the epoch\n",
    "        self.log(\"training_loss\", loss)\n",
    "        for k, v in loss_dict.items():\n",
    "            self.log(\"train_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        self.log(\"validation_loss\", loss)\n",
    "        for k, v in loss_dict.items():\n",
    "            self.log(\"validation_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param_dicts = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.named_parameters()\n",
    "                    if \"backbone\" not in n and p.requires_grad\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.named_parameters()\n",
    "                    if \"backbone\" in n and p.requires_grad\n",
    "                ],\n",
    "                \"lr\": self.lr_backbone,\n",
    "            },\n",
    "        ]\n",
    "        # Feel free to modify the optimizer, I used this because the good performance in other projects and is the frequenly used in transformers models\n",
    "        return torch.optim.AdamW(\n",
    "            param_dicts, lr=self.lr, weight_decay=self.weight_decay\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.training_dataset\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.validation_dataset if self.validation_dataset else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/ralvarez22/Documentos/llm_data/llm_cache/models--facebook--detr-resnet-101/snapshots/7d14702e444d98d0b1764824567fc2b45e1eb218 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DetrForObjectDetection were not initialized from the model checkpoint at /home/ralvarez22/Documentos/llm_data/llm_cache/models--facebook--detr-resnet-101/snapshots/7d14702e444d98d0b1764824567fc2b45e1eb218 and are newly initialized because the shapes did not match:\n",
      "- class_labels_classifier.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- class_labels_classifier.weight: found shape torch.Size([92, 256]) in the checkpoint and torch.Size([3, 256]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create a Pythorch Lightning model\n",
    "model = Detr(\n",
    "    DETR_CHECKPOINT,\n",
    "    lr=MODEL_LR,\n",
    "    lr_backbone=BB_LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    training_dataset=TRAIN_DATALOADER,\n",
    "    validation_dataset=VAL_DATALOADER,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params for the logger\n",
    "hyperparams = {\n",
    "    \"model_type\": \"DETR\",\n",
    "    \"model_name\": \"detr-resnet-101\",\n",
    "    \"codename\": EXPERIMENT_NAME,\n",
    "    \"version\": EXPERIMENT_VERSION,\n",
    "    \"model_learning_rate\": MODEL_LR,\n",
    "    \"backbone_learning_rate\": BB_LR,\n",
    "    \"epochs\": MAX_EPOCHS,\n",
    "    \"weight_decay\": WEIGHT_DECAY,\n",
    "    \"acc_grad_batches\": ACC_BATCH,\n",
    "    \"clip_grad\": CLIP_GRAD,\n",
    "    \"batch_size\": BATCH_SIZE\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 60.441799\n",
      "Trainable params: 60.219399 M\n"
     ]
    }
   ],
   "source": [
    "# To get an idea of the total of params and the trainable params\n",
    "detr_total_params = sum(p.numel() for p in model.parameters())\n",
    "detr_train_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total params: {}\\nTrainable params: {} M\".format(detr_total_params / 1e6, detr_train_params/ 1e6))\n",
    "hyperparams[\"total_params\"] = detr_total_params\n",
    "hyperparams[\"trainable_params\"] = detr_train_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join(LOG_DIR, EXPERIMENT_NAME, \"version_{}\".format(EXPERIMENT_VERSION))\n",
    "shutil.rmtree(log_path, ignore_errors=True)\n",
    "# I used Tensorboard Logger. If you too, please make sure to initiate the TB instance\n",
    "logger = pl.loggers.TensorBoardLogger(save_dir=LOG_DIR, version=EXPERIMENT_VERSION, name=EXPERIMENT_NAME)\n",
    "logger.log_hyperparams(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                   | Params | Mode\n",
      "--------------------------------------------------------\n",
      "0 | model | DetrForObjectDetection | 60.4 M | eval\n",
      "--------------------------------------------------------\n",
      "60.2 M    Trainable params\n",
      "222 K     Non-trainable params\n",
      "60.4 M    Total params\n",
      "241.767   Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "603       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557dddf5be1743c8996f8e089ff6fa3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ralvarez22/server/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "/home/ralvarez22/server/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 15. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/home/ralvarez22/server/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f422ef7509c4717954cb212c91880fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0650f9b5d113431dbbd480771b6632c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ralvarez22/server/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 14. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca56e6a979b4968b31a43096448cd4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n"
     ]
    }
   ],
   "source": [
    "# Create a PL Trainer.\n",
    "# I use 1 device (NVIDIA 4090 24GB). In the first attemp i wanted to use 2 GPUs (4090 & 3060) but the process finishes with error code.\n",
    "# Use the GPU accelerator to speed-up the training and avoid bad performance\n",
    "# accumulate_grad_batches is used to save memory, if you want to experiment, less acc value, more merory used (In theory)\n",
    "trainer = pl.Trainer(devices=1, accelerator=\"gpu\", max_epochs=MAX_EPOCHS, gradient_clip_val=CLIP_GRAD, accumulate_grad_batches=ACC_BATCH, log_every_n_steps=ACC_BATCH, check_val_every_n_epoch=ACC_BATCH, logger=logger)\n",
    "# Run the training Cycle and log the metrics\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, the metrics were the following\n",
    "\n",
    "![DETR Metrics](../metrics/detr_train_metrics.png \"Loss Metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./finetuned/detr/Nous/V_3/preprocessor_config.json']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the final model\n",
    "FINAL_CKP_PATH = os.path.join(CKP_PATH, EXPERIMENT_NAME, \"V_{}\".format(EXPERIMENT_VERSION))\n",
    "# By default it saves the Safetensors type\n",
    "model.model.save_pretrained(FINAL_CKP_PATH)\n",
    "image_processor.save_pretrained(FINAL_CKP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, you can push the model to HF Hub here, but is recommended to first test it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyFormers + CUDA",
   "language": "python",
   "name": "dev-kernel-v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
