{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETR Evaluation Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook allows to evaluate the finetuned model with a portion of the training dataset\n",
    "\n",
    "The only metric for this evaluation is IOU (Intersection over union), because the DETR is only generating bounding boxes, not the label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ralvarez22/server/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/ralvarez22/server/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from transformers import DetrForObjectDetection, DetrImageProcessor\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import cv2\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"../hand-cursive-detr\"\n",
    "ANNOTATION_FILE_NAME = \"_val_annotations_coco.json\"\n",
    "#HF_CACHE = \"/home/ralvarez22/Documentos/llm_data/llm_cache\"\n",
    "DEVICE = \"cuda\"\n",
    "CHECKPOINT = \"../finetuned/detr/Nous/V_1\"\n",
    "CONFIDENCE_TRESHOLD = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FILE = os.path.join(DATASET_DIR, ANNOTATION_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_json = json.load(open(DATASET_FILE, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset_json[\"info\"]\n",
    "del dataset_json[\"licenses\"]\n",
    "del dataset_json[\"categories\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['images', 'annotations'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_json.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_with_boxes = []\n",
    "for e in dataset_json[\"images\"]:\n",
    "    image_annotations = [\n",
    "        x for x in dataset_json[\"annotations\"] if x[\"image_id\"] == e[\"id\"]\n",
    "    ]\n",
    "    images_with_boxes.append(\n",
    "        {\"id\": e[\"id\"], \"image\": e[\"file_name\"], \"boxes\": image_annotations}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#items_to_eval = int(len(images_with_boxes) * 0.9)\n",
    "#images_with_boxes = random.sample(images_with_boxes, items_to_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "detr_proc = DetrImageProcessor.from_pretrained(CHECKPOINT)\n",
    "detr_model = DetrForObjectDetection.from_pretrained(\n",
    "    pretrained_model_name_or_path=CHECKPOINT, ignore_mismatched_sizes=True\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(gt, pred):\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    x_a = max(gt[0], pred[0])\n",
    "    y_a = max(gt[1], pred[1])\n",
    "    x_b = min(gt[2], pred[2])\n",
    "    y_b = min(gt[3], pred[3])\n",
    "    # if there is no overlap between predicted and ground-truth box\n",
    "    if x_b < x_a or y_b < y_a:\n",
    "        return 0.0\n",
    "    # compute the area of intersection rectangle\n",
    "    inter_area = max(0, x_b - x_a + 1) * max(0, y_b - y_a + 1)\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    box_a_area = (gt[2] - gt[0] + 1) * (gt[3] - gt[1] + 1)\n",
    "    box_b_area = (pred[2] - pred[0] + 1) * (pred[3] - pred[1] + 1)\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the intersection area\n",
    "    iou = inter_area / float(box_a_area + box_b_area - inter_area)\n",
    "    # return the intersection over union value\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bboxes(image):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # load image and predict\n",
    "        inputs = detr_proc(images=image, return_tensors='pt').to(DEVICE)\n",
    "        outputs = detr_model(**inputs)\n",
    "\n",
    "        # post-process\n",
    "        target_sizes = torch.tensor([image.shape[:2]]).to(DEVICE)\n",
    "        results = detr_proc.post_process_object_detection(\n",
    "            outputs=outputs, \n",
    "            threshold=CONFIDENCE_TRESHOLD, \n",
    "            target_sizes=target_sizes\n",
    "        )[0]\n",
    "    return results[\"scores\"], results[\"boxes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_iou(target_boxes, generated_boxes):\n",
    "    image_iou = 0\n",
    "    fmt_tgt_boxes = [ [ round(x[0], 4), round(x[1], 4), round(x[0] + x[2], 4), round(x[1] + x[3], 4) ]  for x in target_boxes.tolist()]\n",
    "    pred_boxes = [ [ round(x[0], 4), round(x[1], 4), round(x[2], 4), round(x[3], 4) ] for x in generated_boxes.tolist() ]\n",
    "    total_correct = 0\n",
    "    total_false = 0\n",
    "    false_negatives = 0\n",
    "    for tgt_box in fmt_tgt_boxes:\n",
    "        tgt_iou_items = [ intersection_over_union(tgt_box, x) for x in pred_boxes ]\n",
    "        tgt_iou_items = [ x for x in tgt_iou_items if x > 0 ]\n",
    "        # tgt_iou_items contiene todos los valores con los cuales hay un overlap entre boxes\n",
    "        if len(tgt_iou_items) == 0:\n",
    "            total_false += 1\n",
    "            image_iou += 0\n",
    "            continue\n",
    "        \n",
    "        max_iou_item = max(tgt_iou_items) # Valor maximo de iou para el item\n",
    "        #print(\"IOU for item {} - {}\".format(tgt_box, max_iou_item))\n",
    "        if max_iou_item >= 0.5:\n",
    "            # IOU valido del item\n",
    "            image_iou += max_iou_item\n",
    "            total_correct += 1\n",
    "        elif max_iou_item > 0.3 and max_iou_item < 0.5:\n",
    "            total_false += 1\n",
    "        else:\n",
    "            false_negatives += 1\n",
    "        \n",
    "    return image_iou, total_correct, total_false, false_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ffb6cbb0304e639484070493db1e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/479 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc_iou = 0\n",
    "good_matches = 0\n",
    "poor_matches = 0\n",
    "false_neg = 0\n",
    "total_ds_items = 0\n",
    "for test_item in tqdm(images_with_boxes):\n",
    "    # For every item, I test the expected or ground truth box with all the generated boxes from the model\n",
    "    # The main reason is because the model ignores the 'num_queries' configuration and generates all the posible bounding boxes\n",
    "    gt_boxes = torch.tensor([ x[\"bbox\"] for x in test_item[\"boxes\"] ], device=DEVICE)\n",
    "    total_ds_items += len(gt_boxes)\n",
    "    input_image = os.path.join(DATASET_DIR, test_item[\"image\"])\n",
    "    image_pixels = cv2.imread(input_image)\n",
    "    _, pred_boxes = generate_bboxes(image_pixels)\n",
    "    item_iou, tc, tf, fn = get_image_iou(gt_boxes, pred_boxes)\n",
    "    \n",
    "    acc_iou += item_iou\n",
    "    good_matches += tc\n",
    "    poor_matches += tf\n",
    "    false_neg += fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25910, 838, 638, 0.788608793785049)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_matches, poor_matches, false_neg,  acc_iou / (good_matches + poor_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_prom = acc_iou / (good_matches + poor_matches)\n",
    "prec = good_matches / (poor_matches + good_matches)\n",
    "recall = good_matches / (good_matches + false_neg)\n",
    "f1_score = (2 * prec * recall) / (prec + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOU: 0.7886 - Precision: 0.9687 - Recall: 0.9760 - F1: 0.9723\n"
     ]
    }
   ],
   "source": [
    "print(\"IOU: {:.4f} - Precision: {:.4f} - Recall: {:.4f} - F1: {:.4f}\".format(iou_prom, prec, recall, f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation items: 27386 - Total Correct Items: 25910 - Total Incorrect Items: 838 - Total False Negatives: 638\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Evaluation items: {} - Total Correct Items: {} - Total Incorrect Items: {} - Total False Negatives: {}\".format(total_ds_items, good_matches, poor_matches, false_neg))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyFormers + CUDA",
   "language": "python",
   "name": "dev-kernel-v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
